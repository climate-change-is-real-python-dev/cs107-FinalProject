{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "TRADITIONALLY\n",
      "==================\n",
      "\n",
      "Final step size : 0.32768000000000014\n",
      "==================\n",
      "USING OUR PACAKGE\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#  Backtracking line search algorithm\n",
    "import forward as fw\n",
    "import numpy as np\n",
    "alpha = 0.3\n",
    "beta = 0.8\n",
    "\n",
    "f = lambda x: (x[0]**2 + 3*x[1]*x[0] + 12)\n",
    "\n",
    "print('==================')\n",
    "print('TRADITIONALLY')\n",
    "print('==================')\n",
    "dfx1 = lambda x: (2*x[0] + 3*x[1])\n",
    "dfx2 = lambda x: (3*x[0])\n",
    "\n",
    "\n",
    "\n",
    "t = 1\n",
    "count = 1\n",
    "x0 = np.array([2,3])  # initial guess \n",
    "dx0 = np.array([.1, 0.05])\n",
    "\n",
    "\n",
    "def backtrack(x0, dfx1, dfx2, t, alpha, beta, count):\n",
    "#    t_start = time.monotonic()\n",
    "\n",
    "\n",
    "    while (f(x0) - (f(x0 - t*np.array([dfx1(x0), dfx2(x0)])) + alpha * t * np.dot(np.array([dfx1(x0), dfx2(x0)]), np.array([dfx1(x0), dfx2(x0)])))) < 0:\n",
    "        t *= beta\n",
    "#         print(\"\"\"\n",
    "# ###  iteration {}   ###\n",
    "# \"\"\".format(count))\n",
    "#         print(\"Inequality: \",  f(x0) - (f(x0 - t*np.array([dfx1(x0), dfx2(x0)])) + alpha * t * np.dot(np.array([dfx1(x0), dfx2(x0)]), np.array([dfx1(x0), dfx2(x0)]))))\n",
    "        count += 1\n",
    "#    t_stop = time.monotonic()\n",
    "#    print( f'Time elapsed: {t_stop - t_start} seconds ')\n",
    "    return t\n",
    "\n",
    "t = backtrack(x0, dfx1, dfx2, t, alpha, beta, count)\n",
    "\n",
    "# find the distance \"t\" \n",
    "\n",
    "print(\"\\nFinal step size :\",  t)\n",
    "\n",
    "\n",
    "###############\n",
    "print('==================')\n",
    "print('USING OUR PACAKGE')\n",
    "print('==================')\n",
    "\n",
    "a = np.array([2])\n",
    "x = fw.forwardAD(a, numvar = 2, idx = 0)\n",
    "\n",
    "b = np.array([3]) \n",
    "y = fw.forwardAD(b, numvar = 2, idx = 1)\n",
    "\n",
    "f_AD =(x)**2 + 3*y*x + 12\n",
    "\n",
    "\n",
    "t = 1\n",
    "count = 1\n",
    "x0 = np.array([2,3]) # initial guess \n",
    "\n",
    "\n",
    "\n",
    "def backtrack_AD(variables, x0, t, f, cur_x, alpha, beta, count): # i added varialbes, f, cur_x\n",
    "#    t_start = time.monotonic()\n",
    "    x = fw.forwardAD(x0[0], numvar = 2, idx = 0)\n",
    "    \n",
    "    exec(\"f_ex = lambda x: \" + f)\n",
    "    print(f_ex)    \n",
    "    \n",
    "    if isinstance(f, str):\n",
    "\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x)\n",
    "    \n",
    "        for index, variable in enumerate(variables):\n",
    "\n",
    "            #Store to variable name\n",
    "            name = variable\n",
    "            #print(f'index_var = {index}')\n",
    "            #Save as forwardAD object\n",
    "            exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "            #print(exec(name))\n",
    "        #Stores full function as a forwardAD object\n",
    "            print(f)\n",
    "            AD_function = eval(f)\n",
    "            print('One loop + one variable')\n",
    "#     print('===-----')\n",
    "#     print(AD_function)\n",
    "#     print(len(variables))\n",
    "    print(AD_function.val)\n",
    "    print(AD_function.der)\n",
    "    \n",
    "#     print('=======')\n",
    "#     print(alpha * t * np.dot( f_AD.der, f_AD.der))\n",
    "    \n",
    "#     print(val_prompt)\n",
    "#     exec(val_prompt)\n",
    "#     exec(\"f_AD_der = \" + name + \".val\")\n",
    "    \n",
    "#    print(f_AD_der)\n",
    "    while (AD_function.val - (f_ex(x0 - t* AD_function.der) + alpha * t * np.dot( AD_function.der,  AD_function.der))) < 0:\n",
    "        for index, variable in enumerate(variables):\n",
    "\n",
    "            #Store to variable name\n",
    "            name = variable\n",
    "\n",
    "            #Save as forwardAD object\n",
    "            exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "        \n",
    "        t *= beta\n",
    "        count += 1\n",
    "#    t_stop = time.monotonic()\n",
    "#    print( f'Time elapsed: {t_stop - t_start} seconds ')    \n",
    "    return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t = backtrack_AD(x0, t, alpha, beta, count)\n",
    "# #####################\n",
    "# # def backtrack_AD(variables, x0, t, f, cur_x, alpha, beta, count): # i added varialbes, f, cur_x\n",
    "\n",
    "# variables = \"x\"\n",
    "# f = \"(x+5)**2\"\n",
    "# cur_x = [3]\n",
    "# rate = 0.01\n",
    "# precision = 0.000001\n",
    "# previous_step_size = 1\n",
    "# max_iters = 10000\n",
    "# iters = 0\n",
    "\n",
    "# alpha = 0.8\n",
    "# beta = 0.8\n",
    "# count = 1\n",
    "# initial_guess = [0] \n",
    "\n",
    "# t = backtrack_AD(variables, initial_guess, rate, f, cur_x, alpha, beta, count)\n",
    "\n",
    "# # find the distance \"t\" \n",
    "\n",
    "# print(\"\\nFinal step size :\",  t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters):\n",
    "    if isinstance(f, str):\n",
    "\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x)\n",
    "\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x[index]), numvar = len(variables), idx = index)\")\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "            \n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = np.sum(abs(cur_x - prev_x)) #Change in x\n",
    "            iters += 1 #iteration count\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "    print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "name = 'x'\n",
    "exec(name + \" = 2\")\n",
    "print(eval(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600 \n",
      "X value is [-4.99995648e+00  5.44058269e-06]\n",
      "The local minimum occurs at [-4.99995648e+00  5.44058269e-06]\n"
     ]
    }
   ],
   "source": [
    "variables = [\"x\",\"y\"]\n",
    "f = \"(x+5)**2 + y**2\"\n",
    "cur_x = [3, 1]\n",
    "rate = 0.01\n",
    "precision = 0.000001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fe135136ae8>\n"
     ]
    }
   ],
   "source": [
    "exec(\"f_ex = lambda x: \" + f)\n",
    "print(f_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters):\n",
    "#     if isinstance(f, str):\n",
    "\n",
    "#     #If just one string, convert to list\n",
    "#         if isinstance(variables, str):\n",
    "\n",
    "#             variables = [variables]\n",
    "\n",
    "#         #Convert to numpy arrays\n",
    "#         variables = np.array(variables)\n",
    "#         cur_x = np.array(cur_x)\n",
    "\n",
    "#         iters = 0\n",
    "#         while previous_step_size > precision and iters < max_iters:\n",
    "#             for index, variable in enumerate(variables):\n",
    "\n",
    "#                 #Store to variable name\n",
    "#                 name = variable\n",
    "\n",
    "#                 #Save as forwardAD object\n",
    "#                 exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "#             #Stores full function as a forwardAD object\n",
    "#             AD_function = eval(f)\n",
    "            \n",
    "#             prev_x = cur_x #Store current x value in prev_x\n",
    "#             cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "#             previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "#             iters += 1 #iteration count\n",
    "#             print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "\n",
    "#     print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters):\n",
    "    if isinstance(f, str):\n",
    "\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x)\n",
    "        \n",
    "        f_x = f\n",
    "        \n",
    "        if len(variables) > 1:    \n",
    "            \n",
    "            for idx_var in np.arange(0, len(variables)):\n",
    "                f_x = f_x.replace(variables[idx_var], variables[0] + '[' + str(idx_var) + ']')\n",
    "    \n",
    "        exec(\"f_ex = lambda \" + variables[0] + \":\" + f_x)\n",
    "#        print(f_ex(cur_x))\n",
    "#######################################\n",
    "# ITERATIONS >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x[index]), numvar = len(variables), idx = index)\")\n",
    "#                print(eval(name))\n",
    "                \n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "            \n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            ##### ESTIMATE THE MOST EFFICIENT RATE \n",
    "            t = 1\n",
    "            count = 1\n",
    "            while (AD_function.val - (f_ex(cur_x - t* AD_function.der) + alpha * t * np.dot( AD_function.der,  AD_function.der))) < 0 and count < max_iters:\n",
    "                t *= beta\n",
    "\n",
    "                count += 1\n",
    "            \n",
    "            rate = t\n",
    "            #####\n",
    "            \n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = np.sum(abs(cur_x - prev_x))  #Change in x\n",
    "            iters += 1 #iteration count\n",
    "    #        print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "#######################################\n",
    "    print(\"Iteration\",iters)\n",
    "    print(\"The local minimum occurs at\", cur_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12\n",
      "The local minimum occurs at [-4.99999814e+00  2.32218265e-07]\n"
     ]
    }
   ],
   "source": [
    "variables = [\"x\",\"y\"]\n",
    "f = \"(x+5)**2 + y**2\"\n",
    "cur_x = [3, 1]\n",
    "rate = 0.01\n",
    "precision = 0.00001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"x\",\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ex = lambda x: (x[0] +5)**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ex([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BFGS\n",
    "\n",
    "def BFGS(variables, f, cur_x, precision, max_iters):\n",
    "\n",
    "    if isinstance(f, str):\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x) # points = np.array([2, 2])\n",
    "        f_x = f\n",
    "        \n",
    "        if len(variables) > 1:    \n",
    "            \n",
    "            for idx_var in np.arange(0, len(variables)):\n",
    "                f_x = f_x.replace(variables[idx_var], variables[0] + '[' + str(idx_var) + ']')\n",
    "    \n",
    "        exec(\"f_ex = lambda \" + variables[0] + \":\" + f_x)\n",
    "#######################################        \n",
    "        previous_step_size = 1\n",
    "        p = 1e-8\n",
    "        I = np.identity(len(variables))\n",
    "        Hk = I\n",
    "#######################################\n",
    "# ITERATIONS >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        rate = 0.1\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x[index]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "            prev_x = cur_x\n",
    "            \n",
    "            neg_f_xk =  - AD_function.der\n",
    "            s_k = Hk @ neg_f_xk * 0.0001 # with fix learning rate\n",
    "\n",
    "            cur_x = prev_x + s_k\n",
    "            y_k = f_ex(cur_x) + neg_f_xk\n",
    "    \n",
    "            rho_k = 1/(np.transpose(y_k) @ s_k)\n",
    "            new_Hk = (I - s_k * rho_k @ np.transpose(y_k)) \\\n",
    "            @ Hk @ (I - rho_k * y_k@np.transpose(s_k))\\\n",
    "            + rho_k * s_k@np.transpose(s_k)\n",
    "            Hk = new_Hk\n",
    "\n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            ##### ESTIMATE THE MOST EFFICIENT RATE \n",
    "            #####\n",
    "            #print(rate)\n",
    "            #print(AD_function.der)\n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = np.sum(abs(cur_x - prev_x)) #Change in x\n",
    "            iters += 1 #iteration count\n",
    "            #print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "#######################################\n",
    "    print(\"Iteration\",iters)\n",
    "    print(\"The local minimum occurs at\", cur_x)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56\n",
      "The local minimum occurs at [-4.99997049e+00  3.68814739e-06]\n"
     ]
    }
   ],
   "source": [
    "variables = [\"x\",\"y\"]\n",
    "f = \"(x + 5)**2 + y**2\"\n",
    "cur_x = [3, 1]\n",
    "rate = 0.01\n",
    "precision = 0.00001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "BFGS(variables, f, cur_x, precision, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BFGS\n",
    "\n",
    "def BFGS_backtrack(variables, f, cur_x, precision, max_iters):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(f, str):\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x) # points = np.array([2, 2])\n",
    "       \n",
    "        f_x = f\n",
    "        \n",
    "        if len(variables) > 1:    \n",
    "            \n",
    "            for idx_var in np.arange(0, len(variables)):\n",
    "                f_x = f_x.replace(variables[idx_var], variables[0] + '[' + str(idx_var) + ']')\n",
    "    \n",
    "        exec(\"f_ex = lambda \" + variables[0] + \":\" + f_x)\n",
    "#######################################        \n",
    "        previous_step_size = 1\n",
    "        p = 1e-8\n",
    "        I = np.identity(len(variables))\n",
    "        Hk = I\n",
    "#######################################\n",
    "# ITERATIONS >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        rate = 0.1\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x[index]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            #print(f)\n",
    "            AD_function = eval(f) ## prev step \n",
    "            prev_x = cur_x\n",
    "            \n",
    "            neg_f_xk =  - AD_function.der\n",
    "            s_k = Hk @ neg_f_xk * 0.0001 # with fix learning rate\n",
    "\n",
    "            cur_x = prev_x + s_k\n",
    "            \n",
    "            #######################\n",
    "            ## backtrack_AD\n",
    "            #######################\n",
    "#            t = backtrack_AD(points, 1, alpha, beta, 1)\n",
    "            \n",
    "            t = 1\n",
    "            count = 1\n",
    "           # print('=============')\n",
    "           # print(AD_function.val) #<-- SHOULD BE ONE\n",
    "           # print(cur_x )\n",
    "           # print(- t* AD_function.der)\n",
    "            while (AD_function.val - (f_ex(cur_x - t* AD_function.der) + alpha * t * np.dot( AD_function.der,  AD_function.der))) < 0 and count < max_iters:\n",
    "                t *= beta\n",
    "                count += 1\n",
    "            \n",
    "            #print(f't: {t}')\n",
    "            rate = t\n",
    "            ######################\n",
    "            y_k = f_ex(cur_x) + neg_f_xk * t \n",
    "    \n",
    "            rho_k = 1/(np.transpose(y_k) @ s_k)\n",
    "            new_Hk = (I - s_k * rho_k @ np.transpose(y_k)) \\\n",
    "            @ Hk @ (I - rho_k * y_k@np.transpose(s_k))\\\n",
    "            + rho_k * s_k@np.transpose(s_k)\n",
    "            Hk = new_Hk\n",
    "\n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            ##### ESTIMATE THE MOST EFFICIENT RATE \n",
    "            #####\n",
    "            #print(AD_function.der)\n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size =  np.sum(abs(cur_x - prev_x))  #Change in x\n",
    "            iters += 1 #iteration count\n",
    "            print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "#######################################\n",
    "    print(\"Iteration\",iters)\n",
    "    print(\"The local minimum occurs at\", cur_x)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      "X value is [-7.2416 -0.2802 -0.2802]\n",
      "Iteration 2 \n",
      "X value is [-4.37134329  0.07907243  0.07907243]\n",
      "Iteration 3 \n",
      "X value is [-5.17693641 -0.0229429  -0.0229429 ]\n",
      "Iteration 4 \n",
      "X value is [-4.94948755  0.00736347  0.00736347]\n",
      "Iteration 5 \n",
      "X value is [-5.01526160e+00 -3.17125548e-03 -3.17125548e-03]\n",
      "Iteration 6 \n",
      "X value is [-4.99425151e+00  2.36077370e-03  2.36077370e-03]\n",
      "Iteration 7 \n",
      "X value is [-5.00299541e+00 -2.91342784e-03 -2.91342784e-03]\n",
      "Iteration 8 \n",
      "X value is [-4.99336454  0.00671743  0.00671743]\n",
      "Iteration 8\n",
      "The local minimum occurs at [-4.99336454  0.00671743  0.00671743]\n"
     ]
    }
   ],
   "source": [
    "variables = [\"x\",\"y\", \"z\"]\n",
    "f = \"(x + 5)**2 + y**2 + z**2\"\n",
    "cur_x = [3, 1, 1]\n",
    "rate = 0.01\n",
    "precision = 0.001\n",
    "previous_step_size = 1\n",
    "max_iters = 100\n",
    "iters = 0\n",
    "BFGS_backtrack(variables, f, cur_x, precision, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# points = np.array([2, 2])\n",
    "# iter_n = 0\n",
    "# max_iter = 2000\n",
    "# delta_points = 1\n",
    "# p = 1e-8\n",
    "# I = np.identity(2)\n",
    "# Hk = I\n",
    "\n",
    "# x = forwardAD(points[0], numvar = 2, idx = 0)\n",
    "# y = forwardAD(points[1], numvar = 2, idx = 1)\n",
    "# f_AD =(x)**2  + (y)**2\n",
    "\n",
    "# all_points = []\n",
    "# all_points.append(points)\n",
    "\n",
    "\n",
    "# while (delta_points > p) and (iter_n < max_iter):\n",
    "#     neg_f_xk =  - f_AD.der\n",
    "#     s_k = Hk @ neg_f_xk * 0.0001 # with fix learning rate\n",
    "    \n",
    "#     new_points = points + s_k\n",
    "#     x = forwardAD(new_points[0], numvar = 2, idx = 0)\n",
    "#     y = forwardAD(new_points[1], numvar = 2, idx = 1)\n",
    "#     f_AD =(x)**2  + (y)**2\n",
    "\n",
    "#     y_k = f_AD.der + neg_f_xk\n",
    "#     rho_k = 1/(np.transpose(y_k) @ s_k)\n",
    "#     new_Hk = (I - s_k * rho_k @ np.transpose(y_k)) \\\n",
    "#     @ Hk @ (I - rho_k * y_k@np.transpose(s_k))\\\n",
    "#     + rho_k * s_k@np.transpose(s_k)\n",
    "#     Hk = new_Hk\n",
    "    \n",
    "#     iter_n += 1\n",
    "#     delta_points = np.sqrt((new_points[0] - points[0])**2\\\n",
    "#                            + (new_points[1] - points[1])**2)\n",
    "#     points = new_points\n",
    "#     all_points.append(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Newton function\n",
    "def newton(function, variables, init_values):\n",
    "\n",
    "        #Check user input types (string or list of strings)\n",
    "        if not isinstance(function, str):\n",
    "\n",
    "                #Vector function case\n",
    "                if not isinstance(function,list):\n",
    "                        raise TypeError('function input must be either string or list of strings')\n",
    "\n",
    "                if not isinstance(function[0],str):\n",
    "                        raise TypeError('function input must be either string or list of strings')\n",
    "\n",
    "        #Check user input types (string or list of strings)\n",
    "        if not isinstance(variables, str):\n",
    "\n",
    "                #Vector function case\n",
    "                if not isinstance(variables,list):\n",
    "                        raise TypeError('Each input must be either string or list of strings')\n",
    "\n",
    "                if not isinstance(variables[0],str):\n",
    "                        raise TypeError('Each input must be either string or list of strings')\n",
    "\n",
    "        #Check inital values is a list\n",
    "        if not isinstance(init_values, list):\n",
    "                raise TypeError('initial values must be a list')\n",
    "\n",
    "        #Check that initial values and variables match in length\n",
    "        if not len(variables) == len(init_values):\n",
    "\n",
    "                raise Error('number of initial values must match number of variables')\n",
    "\n",
    "\n",
    "        #Broyden's method for non-vector function\n",
    "        if isinstance(function, str):\n",
    "\n",
    "                #If just one string, convert to list\n",
    "                if isinstance(variables, str):\n",
    "\n",
    "                        variables = [variables]\n",
    "\n",
    "                #Convert to numpy arrays\n",
    "                variables = np.array(variables)\n",
    "                init_values = np.array(init_values)\n",
    "\n",
    "                #Loop through variable names\n",
    "                for index, variable in enumerate(variables):\n",
    "\n",
    "                        #Store to variable name\n",
    "                        name = variable\n",
    "\n",
    "                        #Save as forwardAD object\n",
    "                        exec(name + \" = forwardAD(np.array([init_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                #Stores full function as a forwardAD object\n",
    "                AD_function = eval(function)\n",
    "                \n",
    "#First iteration is Newton's method\n",
    "                new_values = init_values - (AD_function.val / AD_function.der)\n",
    "\n",
    "                #Store init as last values\n",
    "                previous_values = init_values\n",
    "\n",
    "                #Stores function value as previoust function value\n",
    "                previous_function_val = AD_function.val\n",
    "\n",
    "                #Iterate using finite differences until find good enough root\n",
    "                while abs(AD_function.val) > 1e-7:\n",
    "\n",
    "                        #Loop through variable names\n",
    "                        for index, variable in enumerate(variables):\n",
    "\n",
    "                                #Store to variable name\n",
    "                                name = variable\n",
    "\n",
    "                                #Save as forwardAD object\n",
    "                                exec(name + \" = forwardAD(np.array([new_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                        #Stores full function as a forwardAD object\n",
    "                        AD_function = eval(function)\n",
    "\n",
    "                        #Store function value\n",
    "                        previous_function_val = AD_function.val\n",
    "\n",
    "                        #Store domain values\n",
    "                        previous_values = new_values\n",
    "\n",
    "                        #Next estiamte\n",
    "                        new_values = previous_values - (AD_function.val / AD_function.der)\n",
    "\n",
    "                return new_values, AD_function.val\n",
    "\n",
    "        #Case of vector function\n",
    "        else:\n",
    "\n",
    "                #List to store functions\n",
    "                vector = []\n",
    "\n",
    "                #If just one string, convert to list\n",
    "                if isinstance(variables,str):\n",
    "\n",
    "                        variables = [variables]\n",
    "                        \n",
    "#Convert to numpy arrays\n",
    "                variables = np.array(variables)\n",
    "                init_values = np.array(init_values)\n",
    "\n",
    "                #Loop through functions\n",
    "                for func in function:\n",
    "\n",
    "                        #Loop through variable names\n",
    "                        for index, variable in enumerate(variables):\n",
    "\n",
    "                                #Store to variable name\n",
    "                                name = variable\n",
    "\n",
    "                                #Save as forwardAD object\n",
    "                                exec(name + \" = forwardAD(np.array([init_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                        #Stores full function in vector of functions as forwardAD object\n",
    "                        vector.append(eval(func))\n",
    "\n",
    "                #Store as a vector function\n",
    "                AD_vector = vector_func(*vector)\n",
    "\n",
    "                #Store values\n",
    "                previous_values = init_values\n",
    "                previous_values = previous_values.reshape(len(previous_values),1)\n",
    "\n",
    "                #Store jacobian\n",
    "                previous_jacobian = AD_vector.jacobian()\n",
    "\n",
    "                #Store function values\n",
    "                previous_function_val = AD_vector.values()\n",
    "\n",
    "                new_values = previous_values - np.matmul(np.linalg.pinv(AD_vector.jacobian()),AD_vector.values())\n",
    "                new_values = new_values.reshape(1,-1)[0]\n",
    "\n",
    "                #Loop counter\n",
    "                loop = 0\n",
    "\n",
    "                #Iterate using finite differences until find good enough root\n",
    "                while (np.linalg.norm(AD_vector.values()) > 1e-7):\n",
    "\n",
    "                        #List to store functions\n",
    "                        vector = []\n",
    "\n",
    "#Loop through functions\n",
    "                        for func in function:\n",
    "\n",
    "                                #Loop through variable names\n",
    "                                for index, variable in enumerate(variables):\n",
    "\n",
    "                                        #Store to variable name\n",
    "                                        name = variable\n",
    "\n",
    "                                        #Save as forwardAD object\n",
    "                                        exec(name + \" = forwardAD(np.array([new_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                                #Stores full function in vector of functions as forwardAD object\n",
    "                                vector.append(eval(func))\n",
    "\n",
    "                        #Store as a vector function\n",
    "                        AD_vector = vector_func(*vector)\n",
    "\n",
    "                        #Store function value\n",
    "                        previous_function_val = AD_vector.values()\n",
    "\n",
    "                        #Store domain values\n",
    "                        previous_values = new_values\n",
    "                        previous_values = previous_values.reshape(len(previous_values),1)\n",
    "\n",
    "                        #Next estiamte\n",
    "                        new_values = previous_values - np.matmul(np.linalg.pinv(AD_vector.jacobian()),AD_vector.values())\n",
    "                        new_values = new_values.reshape(1,-1)[0]\n",
    "\n",
    "                        if loop > 10000:\n",
    "\n",
    "                                raise ValueError('Too many iterations - could not converge')\n",
    "\n",
    "                        loop += 1\n",
    "\n",
    "                return new_values, AD_vector.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
