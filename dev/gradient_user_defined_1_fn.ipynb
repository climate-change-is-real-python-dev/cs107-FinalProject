{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "TRADITIONALLY\n",
      "==================\n",
      "\n",
      "Final step size : 0.32768000000000014\n",
      "==================\n",
      "USING OUR PACAKGE\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#  Backtracking line search algorithm\n",
    "import forward as fw\n",
    "import numpy as np\n",
    "alpha = 0.3\n",
    "beta = 0.8\n",
    "\n",
    "f = lambda x: (x[0]**2 + 3*x[1]*x[0] + 12)\n",
    "\n",
    "print('==================')\n",
    "print('TRADITIONALLY')\n",
    "print('==================')\n",
    "dfx1 = lambda x: (2*x[0] + 3*x[1])\n",
    "dfx2 = lambda x: (3*x[0])\n",
    "\n",
    "\n",
    "\n",
    "t = 1\n",
    "count = 1\n",
    "x0 = np.array([2,3])  # initial guess \n",
    "dx0 = np.array([.1, 0.05])\n",
    "\n",
    "\n",
    "def backtrack(x0, dfx1, dfx2, t, alpha, beta, count):\n",
    "#    t_start = time.monotonic()\n",
    "\n",
    "\n",
    "    while (f(x0) - (f(x0 - t*np.array([dfx1(x0), dfx2(x0)])) + alpha * t * np.dot(np.array([dfx1(x0), dfx2(x0)]), np.array([dfx1(x0), dfx2(x0)])))) < 0:\n",
    "        t *= beta\n",
    "#         print(\"\"\"\n",
    "# ###  iteration {}   ###\n",
    "# \"\"\".format(count))\n",
    "#         print(\"Inequality: \",  f(x0) - (f(x0 - t*np.array([dfx1(x0), dfx2(x0)])) + alpha * t * np.dot(np.array([dfx1(x0), dfx2(x0)]), np.array([dfx1(x0), dfx2(x0)]))))\n",
    "        count += 1\n",
    "#    t_stop = time.monotonic()\n",
    "#    print( f'Time elapsed: {t_stop - t_start} seconds ')\n",
    "    return t\n",
    "\n",
    "t = backtrack(x0, dfx1, dfx2, t, alpha, beta, count)\n",
    "\n",
    "# find the distance \"t\" \n",
    "\n",
    "print(\"\\nFinal step size :\",  t)\n",
    "\n",
    "\n",
    "###############\n",
    "print('==================')\n",
    "print('USING OUR PACAKGE')\n",
    "print('==================')\n",
    "\n",
    "a = np.array([2])\n",
    "x = fw.forwardAD(a, numvar = 2, idx = 0)\n",
    "\n",
    "b = np.array([3]) \n",
    "y = fw.forwardAD(b, numvar = 2, idx = 1)\n",
    "\n",
    "f_AD =(x)**2 + 3*y*x + 12\n",
    "\n",
    "\n",
    "t = 1\n",
    "count = 1\n",
    "x0 = np.array([2,3]) # initial guess \n",
    "\n",
    "\n",
    "\n",
    "def backtrack_AD(variables, x0, t, f, cur_x, alpha, beta, count): # i added varialbes, f, cur_x\n",
    "#    t_start = time.monotonic()\n",
    "    x = fw.forwardAD(x0[0], numvar = 2, idx = 0)\n",
    "    \n",
    "    exec(\"f_ex = lambda x: \" + f)\n",
    "    print(f_ex)    \n",
    "    \n",
    "    if isinstance(f, str):\n",
    "\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x)\n",
    "    \n",
    "        for index, variable in enumerate(variables):\n",
    "\n",
    "            #Store to variable name\n",
    "            name = variable\n",
    "            #print(f'index_var = {index}')\n",
    "            #Save as forwardAD object\n",
    "            exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "            #print(exec(name))\n",
    "        #Stores full function as a forwardAD object\n",
    "            print(f)\n",
    "            AD_function = eval(f)\n",
    "            print('One loop + one variable')\n",
    "#     print('===-----')\n",
    "#     print(AD_function)\n",
    "#     print(len(variables))\n",
    "    print(AD_function.val)\n",
    "    print(AD_function.der)\n",
    "    \n",
    "#     print('=======')\n",
    "#     print(alpha * t * np.dot( f_AD.der, f_AD.der))\n",
    "    \n",
    "#     print(val_prompt)\n",
    "#     exec(val_prompt)\n",
    "#     exec(\"f_AD_der = \" + name + \".val\")\n",
    "    \n",
    "#    print(f_AD_der)\n",
    "    while (AD_function.val - (f_ex(x0 - t* AD_function.der) + alpha * t * np.dot( AD_function.der,  AD_function.der))) < 0:\n",
    "        for index, variable in enumerate(variables):\n",
    "\n",
    "            #Store to variable name\n",
    "            name = variable\n",
    "\n",
    "            #Save as forwardAD object\n",
    "            exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "        \n",
    "        t *= beta\n",
    "        count += 1\n",
    "#    t_stop = time.monotonic()\n",
    "#    print( f'Time elapsed: {t_stop - t_start} seconds ')    \n",
    "    return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t = backtrack_AD(x0, t, alpha, beta, count)\n",
    "# #####################\n",
    "# # def backtrack_AD(variables, x0, t, f, cur_x, alpha, beta, count): # i added varialbes, f, cur_x\n",
    "\n",
    "# variables = \"x\"\n",
    "# f = \"(x+5)**2\"\n",
    "# cur_x = [3]\n",
    "# rate = 0.01\n",
    "# precision = 0.000001\n",
    "# previous_step_size = 1\n",
    "# max_iters = 10000\n",
    "# iters = 0\n",
    "\n",
    "# alpha = 0.8\n",
    "# beta = 0.8\n",
    "# count = 1\n",
    "# initial_guess = [0] \n",
    "\n",
    "# t = backtrack_AD(variables, initial_guess, rate, f, cur_x, alpha, beta, count)\n",
    "\n",
    "# # find the distance \"t\" \n",
    "\n",
    "# print(\"\\nFinal step size :\",  t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters):\n",
    "    if isinstance(f, str):\n",
    "\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x)\n",
    "\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "            \n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "            iters += 1 #iteration count\n",
    "            \n",
    "    print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "    print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 595 \n",
      "X value is [-4.99995185]\n",
      "The local minimum occurs at [-4.99995185]\n"
     ]
    }
   ],
   "source": [
    "variables = \"x\"\n",
    "f = \"(x+5)**2\"\n",
    "cur_x = [3]\n",
    "rate = 0.01\n",
    "precision = 0.000001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fe135136ae8>\n"
     ]
    }
   ],
   "source": [
    "exec(\"f_ex = lambda x: \" + f)\n",
    "print(f_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters):\n",
    "#     if isinstance(f, str):\n",
    "\n",
    "#     #If just one string, convert to list\n",
    "#         if isinstance(variables, str):\n",
    "\n",
    "#             variables = [variables]\n",
    "\n",
    "#         #Convert to numpy arrays\n",
    "#         variables = np.array(variables)\n",
    "#         cur_x = np.array(cur_x)\n",
    "\n",
    "#         iters = 0\n",
    "#         while previous_step_size > precision and iters < max_iters:\n",
    "#             for index, variable in enumerate(variables):\n",
    "\n",
    "#                 #Store to variable name\n",
    "#                 name = variable\n",
    "\n",
    "#                 #Save as forwardAD object\n",
    "#                 exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "#             #Stores full function as a forwardAD object\n",
    "#             AD_function = eval(f)\n",
    "            \n",
    "#             prev_x = cur_x #Store current x value in prev_x\n",
    "#             cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "#             previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "#             iters += 1 #iteration count\n",
    "#             print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "\n",
    "#     print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters):\n",
    "    if isinstance(f, str):\n",
    "\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x)\n",
    "        exec(\"f_ex = lambda x: \" + f)\n",
    "#######################################\n",
    "# ITERATIONS >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "            \n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            ##### ESTIMATE THE MOST EFFICIENT RATE \n",
    "            t = 1\n",
    "            while (AD_function.val - (f_ex(cur_x - t* AD_function.der) + alpha * t * np.dot( AD_function.der,  AD_function.der))) < 0:\n",
    "                t *= beta\n",
    "                #count += 1\n",
    "            \n",
    "            rate = t\n",
    "            #####\n",
    "            \n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "            iters += 1 #iteration count\n",
    "    #        print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "#######################################\n",
    "    print(\"Iteration\",iters)\n",
    "    print(\"The local minimum occurs at\", cur_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12\n",
      "The local minimum occurs at [-4.99999814]\n"
     ]
    }
   ],
   "source": [
    "variables = \"x\"\n",
    "f = \"(x+5)**2\"\n",
    "cur_x = [3]\n",
    "rate = 0.01\n",
    "precision = 0.00001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BFGS\n",
    "\n",
    "def BFGS(variables, f, cur_x, precision, max_iters):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(f, str):\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x) # points = np.array([2, 2])\n",
    "        exec(\"f_ex = lambda x: \" + f)\n",
    "#######################################        \n",
    "        previous_step_size = 1\n",
    "        p = 1e-8\n",
    "        I = np.identity(len(variables))\n",
    "        Hk = I\n",
    "#######################################\n",
    "# ITERATIONS >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        rate = 0.1\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f)\n",
    "            prev_x = cur_x\n",
    "            \n",
    "            neg_f_xk =  - AD_function.der\n",
    "            s_k = Hk @ neg_f_xk * 0.0001 # with fix learning rate\n",
    "\n",
    "            cur_x = prev_x + s_k\n",
    "            y_k = f_ex(cur_x) + neg_f_xk\n",
    "    \n",
    "            rho_k = 1/(np.transpose(y_k) @ s_k)\n",
    "            new_Hk = (I - s_k * rho_k @ np.transpose(y_k)) \\\n",
    "            @ Hk @ (I - rho_k * y_k@np.transpose(s_k))\\\n",
    "            + rho_k * s_k@np.transpose(s_k)\n",
    "            Hk = new_Hk\n",
    "\n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            ##### ESTIMATE THE MOST EFFICIENT RATE \n",
    "            #####\n",
    "            print(rate)\n",
    "            print(AD_function.der)\n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "            iters += 1 #iteration count\n",
    "            print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "#######################################\n",
    "    print(\"Iteration\",iters)\n",
    "    print(\"The local minimum occurs at\", cur_x)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "[16.]\n",
      "Iteration 1 \n",
      "X value is [1.3984]\n",
      "0.1\n",
      "[12.7968]\n",
      "Iteration 2 \n",
      "X value is [0.11872004]\n",
      "0.1\n",
      "[10.23744009]\n",
      "Iteration 3 \n",
      "X value is [-0.90502397]\n",
      "0.1\n",
      "[8.18995207]\n",
      "Iteration 4 \n",
      "X value is [-1.72401917]\n",
      "0.1\n",
      "[6.55196165]\n",
      "Iteration 5 \n",
      "X value is [-2.37921534]\n",
      "0.1\n",
      "[5.24156932]\n",
      "Iteration 6 \n",
      "X value is [-2.90337227]\n",
      "0.1\n",
      "[4.19325546]\n",
      "Iteration 7 \n",
      "X value is [-3.32269782]\n",
      "0.1\n",
      "[3.35460437]\n",
      "Iteration 8 \n",
      "X value is [-3.65815825]\n",
      "0.1\n",
      "[2.68368349]\n",
      "Iteration 9 \n",
      "X value is [-3.9265266]\n",
      "0.1\n",
      "[2.14694679]\n",
      "Iteration 10 \n",
      "X value is [-4.14122128]\n",
      "0.1\n",
      "[1.71755744]\n",
      "Iteration 11 \n",
      "X value is [-4.31297703]\n",
      "0.1\n",
      "[1.37404595]\n",
      "Iteration 12 \n",
      "X value is [-4.45038162]\n",
      "0.1\n",
      "[1.09923676]\n",
      "Iteration 13 \n",
      "X value is [-4.5603053]\n",
      "0.1\n",
      "[0.87938941]\n",
      "Iteration 14 \n",
      "X value is [-4.64824424]\n",
      "0.1\n",
      "[0.70351153]\n",
      "Iteration 15 \n",
      "X value is [-4.71859539]\n",
      "0.1\n",
      "[0.56280922]\n",
      "Iteration 16 \n",
      "X value is [-4.77487631]\n",
      "0.1\n",
      "[0.45024738]\n",
      "Iteration 17 \n",
      "X value is [-4.81990105]\n",
      "0.1\n",
      "[0.3601979]\n",
      "Iteration 18 \n",
      "X value is [-4.85592084]\n",
      "0.1\n",
      "[0.28815832]\n",
      "Iteration 19 \n",
      "X value is [-4.88473667]\n",
      "0.1\n",
      "[0.23052666]\n",
      "Iteration 20 \n",
      "X value is [-4.90778934]\n",
      "0.1\n",
      "[0.18442133]\n",
      "Iteration 21 \n",
      "X value is [-4.92623147]\n",
      "0.1\n",
      "[0.14753706]\n",
      "Iteration 22 \n",
      "X value is [-4.94098518]\n",
      "0.1\n",
      "[0.11802965]\n",
      "Iteration 23 \n",
      "X value is [-4.95278814]\n",
      "0.1\n",
      "[0.09442372]\n",
      "Iteration 24 \n",
      "X value is [-4.96223051]\n",
      "0.1\n",
      "[0.07553897]\n",
      "Iteration 25 \n",
      "X value is [-4.96978441]\n",
      "0.1\n",
      "[0.06043118]\n",
      "Iteration 26 \n",
      "X value is [-4.97582753]\n",
      "0.1\n",
      "[0.04834494]\n",
      "Iteration 27 \n",
      "X value is [-4.98066202]\n",
      "0.1\n",
      "[0.03867596]\n",
      "Iteration 28 \n",
      "X value is [-4.98452962]\n",
      "0.1\n",
      "[0.03094076]\n",
      "Iteration 29 \n",
      "X value is [-4.98762369]\n",
      "0.1\n",
      "[0.02475261]\n",
      "Iteration 30 \n",
      "X value is [-4.99009896]\n",
      "0.1\n",
      "[0.01980209]\n",
      "Iteration 31 \n",
      "X value is [-4.99207916]\n",
      "0.1\n",
      "[0.01584167]\n",
      "Iteration 32 \n",
      "X value is [-4.99366333]\n",
      "0.1\n",
      "[0.01267334]\n",
      "Iteration 33 \n",
      "X value is [-4.99493067]\n",
      "0.1\n",
      "[0.01013867]\n",
      "Iteration 34 \n",
      "X value is [-4.99594453]\n",
      "0.1\n",
      "[0.00811094]\n",
      "Iteration 35 \n",
      "X value is [-4.99675563]\n",
      "0.1\n",
      "[0.00648875]\n",
      "Iteration 36 \n",
      "X value is [-4.9974045]\n",
      "0.1\n",
      "[0.005191]\n",
      "Iteration 37 \n",
      "X value is [-4.9979236]\n",
      "0.1\n",
      "[0.0041528]\n",
      "Iteration 38 \n",
      "X value is [-4.99833888]\n",
      "0.1\n",
      "[0.00332224]\n",
      "Iteration 39 \n",
      "X value is [-4.9986711]\n",
      "0.1\n",
      "[0.00265779]\n",
      "Iteration 40 \n",
      "X value is [-4.99893688]\n",
      "0.1\n",
      "[0.00212623]\n",
      "Iteration 41 \n",
      "X value is [-4.99914951]\n",
      "0.1\n",
      "[0.00170099]\n",
      "Iteration 42 \n",
      "X value is [-4.99931961]\n",
      "0.1\n",
      "[0.00136079]\n",
      "Iteration 43 \n",
      "X value is [-4.99945568]\n",
      "0.1\n",
      "[0.00108863]\n",
      "Iteration 44 \n",
      "X value is [-4.99956455]\n",
      "0.1\n",
      "[0.00087091]\n",
      "Iteration 45 \n",
      "X value is [-4.99965164]\n",
      "0.1\n",
      "[0.00069672]\n",
      "Iteration 46 \n",
      "X value is [-4.99972131]\n",
      "0.1\n",
      "[0.00055738]\n",
      "Iteration 47 \n",
      "X value is [-4.99977705]\n",
      "0.1\n",
      "[0.0004459]\n",
      "Iteration 48 \n",
      "X value is [-4.99982164]\n",
      "0.1\n",
      "[0.00035672]\n",
      "Iteration 49 \n",
      "X value is [-4.99985731]\n",
      "0.1\n",
      "[0.00028538]\n",
      "Iteration 50 \n",
      "X value is [-4.99988585]\n",
      "0.1\n",
      "[0.0002283]\n",
      "Iteration 51 \n",
      "X value is [-4.99990868]\n",
      "0.1\n",
      "[0.00018264]\n",
      "Iteration 52 \n",
      "X value is [-4.99992694]\n",
      "0.1\n",
      "[0.00014611]\n",
      "Iteration 53 \n",
      "X value is [-4.99994155]\n",
      "0.1\n",
      "[0.00011689]\n",
      "Iteration 54 \n",
      "X value is [-4.99995324]\n",
      "0.1\n",
      "[9.35127215e-05]\n",
      "Iteration 55 \n",
      "X value is [-4.99996259]\n",
      "Iteration 55\n",
      "The local minimum occurs at [-4.99996259]\n"
     ]
    }
   ],
   "source": [
    "variables = \"x\"\n",
    "f = \"(x+5)**2\"\n",
    "cur_x = [3]\n",
    "rate = 0.01\n",
    "precision = 0.00001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "BFGS(variables, f, cur_x, precision, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BFGS\n",
    "\n",
    "def BFGS_backtrack(variables, f, cur_x, precision, max_iters):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(f, str):\n",
    "    #If just one string, convert to list\n",
    "        if isinstance(variables, str):\n",
    "\n",
    "            variables = [variables]\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        variables = np.array(variables)\n",
    "        cur_x = np.array(cur_x) # points = np.array([2, 2])\n",
    "        exec(\"f_ex = lambda x: \" + f)\n",
    "#######################################        \n",
    "        previous_step_size = 1\n",
    "        p = 1e-8\n",
    "        I = np.identity(len(variables))\n",
    "        Hk = I\n",
    "#######################################\n",
    "# ITERATIONS >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        rate = 0.1\n",
    "        iters = 0\n",
    "        while previous_step_size > precision and iters < max_iters:\n",
    "            for index, variable in enumerate(variables):\n",
    "\n",
    "                #Store to variable name\n",
    "                name = variable\n",
    "\n",
    "                #Save as forwardAD object\n",
    "                exec(name + \" = fw.forwardAD(np.array(cur_x), numvar = len(variables), idx = index)\")\n",
    "\n",
    "            #Stores full function as a forwardAD object\n",
    "            AD_function = eval(f) ## prev step \n",
    "            prev_x = cur_x\n",
    "            \n",
    "            neg_f_xk =  - AD_function.der\n",
    "            s_k = Hk @ neg_f_xk * 0.0001 # with fix learning rate\n",
    "\n",
    "            cur_x = prev_x + s_k\n",
    "            \n",
    "            #######################\n",
    "            ## backtrack_AD\n",
    "            #######################\n",
    "#            t = backtrack_AD(points, 1, alpha, beta, 1)\n",
    "            \n",
    "            t = 1\n",
    "            while (AD_function.val - (f_ex(cur_x - t* AD_function.der) + alpha * t * np.dot( AD_function.der,  AD_function.der))) < 0:\n",
    "                t *= beta\n",
    "                #count += 1\n",
    "            \n",
    "            print(f't: {t}')\n",
    "            rate = t\n",
    "            ######################\n",
    "            y_k = f_ex(cur_x) + neg_f_xk * t \n",
    "    \n",
    "            rho_k = 1/(np.transpose(y_k) @ s_k)\n",
    "            new_Hk = (I - s_k * rho_k @ np.transpose(y_k)) \\\n",
    "            @ Hk @ (I - rho_k * y_k@np.transpose(s_k))\\\n",
    "            + rho_k * s_k@np.transpose(s_k)\n",
    "            Hk = new_Hk\n",
    "\n",
    "            prev_x = cur_x #Store current x value in prev_x\n",
    "            ##### ESTIMATE THE MOST EFFICIENT RATE \n",
    "            #####\n",
    "            print(AD_function.der)\n",
    "            cur_x = cur_x - rate * AD_function.der #Grad descent\n",
    "            previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "            iters += 1 #iteration count\n",
    "            print(\"Iteration\",iters,\"\\nX value is\", cur_x) #Print iterations\n",
    "#######################################\n",
    "    print(\"Iteration\",iters)\n",
    "    print(\"The local minimum occurs at\", cur_x)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0.6400000000000001\n",
      "[16.]\n",
      "Iteration 1 \n",
      "X value is [-7.2416]\n",
      "t: 0.6400000000000001\n",
      "[-4.4832]\n",
      "Iteration 2 \n",
      "X value is [-4.37235201]\n",
      "t: 0.6400000000000001\n",
      "[1.25529597]\n",
      "Iteration 3 \n",
      "X value is [-5.17574144]\n",
      "t: 0.6400000000000001\n",
      "[-0.35148287]\n",
      "Iteration 4 \n",
      "X value is [-4.9507924]\n",
      "t: 0.6400000000000001\n",
      "[0.0984152]\n",
      "Iteration 5 \n",
      "X value is [-5.01377813]\n",
      "t: 0.6400000000000001\n",
      "[-0.02755626]\n",
      "Iteration 6 \n",
      "X value is [-4.99614212]\n",
      "t: 0.6400000000000001\n",
      "[0.00771575]\n",
      "Iteration 7 \n",
      "X value is [-5.00108021]\n",
      "t: 0.6400000000000001\n",
      "[-0.00216041]\n",
      "Iteration 8 \n",
      "X value is [-4.99969754]\n",
      "t: 0.6400000000000001\n",
      "[0.00060491]\n",
      "Iteration 9 \n",
      "X value is [-5.00008469]\n",
      "t: 0.6400000000000001\n",
      "[-0.00016938]\n",
      "Iteration 10 \n",
      "X value is [-4.99997629]\n",
      "t: 0.6400000000000001\n",
      "[4.74253327e-05]\n",
      "Iteration 11 \n",
      "X value is [-5.00000664]\n",
      "t: 0.6400000000000001\n",
      "[-1.32790932e-05]\n",
      "Iteration 12 \n",
      "X value is [-4.99999814]\n",
      "Iteration 12\n",
      "The local minimum occurs at [-4.99999814]\n"
     ]
    }
   ],
   "source": [
    "variables = \"x\"\n",
    "f = \"(x+5)**2\"\n",
    "cur_x = [3]\n",
    "rate = 0.01\n",
    "precision = 0.00001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "BFGS_backtrack(variables, f, cur_x, precision, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# points = np.array([2, 2])\n",
    "# iter_n = 0\n",
    "# max_iter = 2000\n",
    "# delta_points = 1\n",
    "# p = 1e-8\n",
    "# I = np.identity(2)\n",
    "# Hk = I\n",
    "\n",
    "# x = forwardAD(points[0], numvar = 2, idx = 0)\n",
    "# y = forwardAD(points[1], numvar = 2, idx = 1)\n",
    "# f_AD =(x)**2  + (y)**2\n",
    "\n",
    "# all_points = []\n",
    "# all_points.append(points)\n",
    "\n",
    "\n",
    "# while (delta_points > p) and (iter_n < max_iter):\n",
    "#     neg_f_xk =  - f_AD.der\n",
    "#     s_k = Hk @ neg_f_xk * 0.0001 # with fix learning rate\n",
    "    \n",
    "#     new_points = points + s_k\n",
    "#     x = forwardAD(new_points[0], numvar = 2, idx = 0)\n",
    "#     y = forwardAD(new_points[1], numvar = 2, idx = 1)\n",
    "#     f_AD =(x)**2  + (y)**2\n",
    "\n",
    "#     y_k = f_AD.der + neg_f_xk\n",
    "#     rho_k = 1/(np.transpose(y_k) @ s_k)\n",
    "#     new_Hk = (I - s_k * rho_k @ np.transpose(y_k)) \\\n",
    "#     @ Hk @ (I - rho_k * y_k@np.transpose(s_k))\\\n",
    "#     + rho_k * s_k@np.transpose(s_k)\n",
    "#     Hk = new_Hk\n",
    "    \n",
    "#     iter_n += 1\n",
    "#     delta_points = np.sqrt((new_points[0] - points[0])**2\\\n",
    "#                            + (new_points[1] - points[1])**2)\n",
    "#     points = new_points\n",
    "#     all_points.append(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Newton function\n",
    "def newton(function, variables, init_values):\n",
    "\n",
    "        #Check user input types (string or list of strings)\n",
    "        if not isinstance(function, str):\n",
    "\n",
    "                #Vector function case\n",
    "                if not isinstance(function,list):\n",
    "                        raise TypeError('function input must be either string or list of strings')\n",
    "\n",
    "                if not isinstance(function[0],str):\n",
    "                        raise TypeError('function input must be either string or list of strings')\n",
    "\n",
    "        #Check user input types (string or list of strings)\n",
    "        if not isinstance(variables, str):\n",
    "\n",
    "                #Vector function case\n",
    "                if not isinstance(variables,list):\n",
    "                        raise TypeError('Each input must be either string or list of strings')\n",
    "\n",
    "                if not isinstance(variables[0],str):\n",
    "                        raise TypeError('Each input must be either string or list of strings')\n",
    "\n",
    "        #Check inital values is a list\n",
    "        if not isinstance(init_values, list):\n",
    "                raise TypeError('initial values must be a list')\n",
    "\n",
    "        #Check that initial values and variables match in length\n",
    "        if not len(variables) == len(init_values):\n",
    "\n",
    "                raise Error('number of initial values must match number of variables')\n",
    "\n",
    "\n",
    "        #Broyden's method for non-vector function\n",
    "        if isinstance(function, str):\n",
    "\n",
    "                #If just one string, convert to list\n",
    "                if isinstance(variables, str):\n",
    "\n",
    "                        variables = [variables]\n",
    "\n",
    "                #Convert to numpy arrays\n",
    "                variables = np.array(variables)\n",
    "                init_values = np.array(init_values)\n",
    "\n",
    "                #Loop through variable names\n",
    "                for index, variable in enumerate(variables):\n",
    "\n",
    "                        #Store to variable name\n",
    "                        name = variable\n",
    "\n",
    "                        #Save as forwardAD object\n",
    "                        exec(name + \" = forwardAD(np.array([init_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                #Stores full function as a forwardAD object\n",
    "                AD_function = eval(function)\n",
    "                \n",
    "#First iteration is Newton's method\n",
    "                new_values = init_values - (AD_function.val / AD_function.der)\n",
    "\n",
    "                #Store init as last values\n",
    "                previous_values = init_values\n",
    "\n",
    "                #Stores function value as previoust function value\n",
    "                previous_function_val = AD_function.val\n",
    "\n",
    "                #Iterate using finite differences until find good enough root\n",
    "                while abs(AD_function.val) > 1e-7:\n",
    "\n",
    "                        #Loop through variable names\n",
    "                        for index, variable in enumerate(variables):\n",
    "\n",
    "                                #Store to variable name\n",
    "                                name = variable\n",
    "\n",
    "                                #Save as forwardAD object\n",
    "                                exec(name + \" = forwardAD(np.array([new_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                        #Stores full function as a forwardAD object\n",
    "                        AD_function = eval(function)\n",
    "\n",
    "                        #Store function value\n",
    "                        previous_function_val = AD_function.val\n",
    "\n",
    "                        #Store domain values\n",
    "                        previous_values = new_values\n",
    "\n",
    "                        #Next estiamte\n",
    "                        new_values = previous_values - (AD_function.val / AD_function.der)\n",
    "\n",
    "                return new_values, AD_function.val\n",
    "\n",
    "        #Case of vector function\n",
    "        else:\n",
    "\n",
    "                #List to store functions\n",
    "                vector = []\n",
    "\n",
    "                #If just one string, convert to list\n",
    "                if isinstance(variables,str):\n",
    "\n",
    "                        variables = [variables]\n",
    "                        \n",
    "#Convert to numpy arrays\n",
    "                variables = np.array(variables)\n",
    "                init_values = np.array(init_values)\n",
    "\n",
    "                #Loop through functions\n",
    "                for func in function:\n",
    "\n",
    "                        #Loop through variable names\n",
    "                        for index, variable in enumerate(variables):\n",
    "\n",
    "                                #Store to variable name\n",
    "                                name = variable\n",
    "\n",
    "                                #Save as forwardAD object\n",
    "                                exec(name + \" = forwardAD(np.array([init_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                        #Stores full function in vector of functions as forwardAD object\n",
    "                        vector.append(eval(func))\n",
    "\n",
    "                #Store as a vector function\n",
    "                AD_vector = vector_func(*vector)\n",
    "\n",
    "                #Store values\n",
    "                previous_values = init_values\n",
    "                previous_values = previous_values.reshape(len(previous_values),1)\n",
    "\n",
    "                #Store jacobian\n",
    "                previous_jacobian = AD_vector.jacobian()\n",
    "\n",
    "                #Store function values\n",
    "                previous_function_val = AD_vector.values()\n",
    "\n",
    "                new_values = previous_values - np.matmul(np.linalg.pinv(AD_vector.jacobian()),AD_vector.values())\n",
    "                new_values = new_values.reshape(1,-1)[0]\n",
    "\n",
    "                #Loop counter\n",
    "                loop = 0\n",
    "\n",
    "                #Iterate using finite differences until find good enough root\n",
    "                while (np.linalg.norm(AD_vector.values()) > 1e-7):\n",
    "\n",
    "                        #List to store functions\n",
    "                        vector = []\n",
    "\n",
    "#Loop through functions\n",
    "                        for func in function:\n",
    "\n",
    "                                #Loop through variable names\n",
    "                                for index, variable in enumerate(variables):\n",
    "\n",
    "                                        #Store to variable name\n",
    "                                        name = variable\n",
    "\n",
    "                                        #Save as forwardAD object\n",
    "                                        exec(name + \" = forwardAD(np.array([new_values[index]]), numvar = len(variables), idx = index)\")\n",
    "\n",
    "                                #Stores full function in vector of functions as forwardAD object\n",
    "                                vector.append(eval(func))\n",
    "\n",
    "                        #Store as a vector function\n",
    "                        AD_vector = vector_func(*vector)\n",
    "\n",
    "                        #Store function value\n",
    "                        previous_function_val = AD_vector.values()\n",
    "\n",
    "                        #Store domain values\n",
    "                        previous_values = new_values\n",
    "                        previous_values = previous_values.reshape(len(previous_values),1)\n",
    "\n",
    "                        #Next estiamte\n",
    "                        new_values = previous_values - np.matmul(np.linalg.pinv(AD_vector.jacobian()),AD_vector.values())\n",
    "                        new_values = new_values.reshape(1,-1)[0]\n",
    "\n",
    "                        if loop > 10000:\n",
    "\n",
    "                                raise ValueError('Too many iterations - could not converge')\n",
    "\n",
    "                        loop += 1\n",
    "\n",
    "                return new_values, AD_vector.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
