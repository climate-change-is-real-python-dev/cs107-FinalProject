{"cells":[{"cell_type":"markdown","source":"## Introduction\nThe software aims to provide users with functions that solve derivatives using automatic differentiation. Solving derivatives is an important task in many fields, as derivatives allow for the optimization of functions and for precise descriptions of rates of change. For example, engineers use derivatives to optimize space given material constraints, physicists use derivatives to map particle motion, and economists use derivatives to describe changing markets.\n\nWe will implement automatic differentiation (AD). AD is a powerful method, as it allows users to solve derivatives for highly complex functions efficiently and with machine precision.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00001-f654ad6a-008b-4b21-ba58-b79505dfdfca"}},{"cell_type":"markdown","source":"## Background\n\nThe chain rule is used to evaluate the derivative of a function by differentiating the outer function, multiplied by the derivative of the inner function, until all layers of the function are complete.\n\nSuppose we have a function $F(x) = f(g(x))$, then $F'(x) = f'(g(x))g'(x)$.\n\nUsing Leibniz notation, the chain rule is represented as:\n\n$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$\n\n\nWe can traverse a function $f(x)$ by following what happens to its input, $x$. We evaluate from the innermost layer and work our way outwards. For each layer, we calculate its derivative and then feed that as the input to the next layer.\n\nElementary functions (_eg._ $sin$, $cos$, $exp$, $log$) are common in AD applications. These functions usually differentiate to fixed solutions; therefore, we implement methods that evaluate these functions.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-6b8a6822-8663-4b7c-ad5d-80c2b69ab4c0"}},{"cell_type":"markdown","source":"## How to use socialAD\n\nUsers can simply download our open-source package _socialAD_ via pip. \n\n\n```\npip install socialAD\n```\n\nThen, to get the forward mode implementation in you work environment, use `from socialAD.forward import *`. Importing numpy is also required.\n\nTo instantiate AD objects, we can simply call `forwardAD(val)` with an initial function input value.\n\nHere is a quick demonstration of using the forward mode to solve the derivative of the function $$f(x) = e^{x} - (2 - 6x - 3x^{5})$$ at x = 5. Note: all multiplication must be made explicit.\nFor more information on writing functions for our package, see the \"Elementary function conventions\" section below.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00003-51911732-2c12-4dcb-9f21-2b2939bb6783"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-4c4bb330-d366-4e68-8a5d-ef07ffac81a5","output_cleared":false,"source_hash":"e2892992","execution_millis":5,"execution_start":1605732291896},"source":"from socialAD.forward import *\nimport numpy as np\n\n#Instantiate initial input to function\nx = forwardAD(5)\n\n#Function (note: make multiplication explicit)\nf = e()**x - (2 - 6*x - 3 * x ** 5)\n\n#Get function value at x = 5 and derivative at x = 5\nprint('Function value: ', f.val, '\\nDerivative value: ', f.der)","execution_count":null,"outputs":[{"name":"stdout","text":"Function value:  9551.413159102576 \nDerivative value:  9529.413159102576\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here is a demonstration of finding a root for that function using Newton's Method - our package is used for finding the derivatives within Newton's Method:","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-76aca3eb-b7d5-4dcb-a712-868900eebb36"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-612c8976-1f72-4e69-b16c-0682b5220260","output_cleared":false,"source_hash":"f79e1319","execution_millis":2,"execution_start":1605732351896},"source":"#Initialize root value and step value\nroot = 0.0\ndx = 50\n\n#While step is greater than a very small number\nwhile dx > 1e-20:\n\n    #Instantiate a forward AD object at root guess\n\tx = forwardAD(root)\n\n    #Write our function\n\tf = e()**x - (2 - 6*x - 3 * x ** 5)\n\n    #Get step using Newton's Method\n\tdx = -f.val / f.der\n\n    #Save new root with new step\n\troot = root + dx\n    \nprint('Root value: ', root, '\\nFunction value at root: ', f.val)","execution_count":null,"outputs":[{"name":"stdout","text":"Root value:  0.14133666319971333 \nFunction value at root:  0.010886349092763536\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Software Organization\n\n\n**Directory Structure:**\n\n`docs`\n\n- Includes documentation, including these subdirectories:\n\n`examples` ** has not been implemented yet\n\n- Includes example use in social science, including these subdirectories:\n\n- `simple_chain_rule`\n\n- `economic_rate_of_change`\n\n- `social_growth_rate`\n\n`socialAD`\n\n- Includes all install and method materials\n- So far, we have `forward.py`\n\n`tests`\n- Include all the tests (currently, the most relevant is: `forwardAD_tests.py`)\n\nREADME.md\n\n- Includes install instructions, author names, and CodeCov/TravisCI buttons\n\nrequirements.txt\n\n- Lists requirements for being able to successfully use our package\n\n\n**Here are the modules and their basic functionality**\n- forward: computes derivative using forward mode of automatic differentiation\n- social: computes maximum or minimum of a loss or objective function for applications in social science\n\n**Our suite live is tested on both TravisCI and CodeCov**\nPlease check `.travis.yml` to see how the tests were performed. The big picture us the suite is built on Travis, and CodeCov runs the testing scripts in the `tests` folder. CodeCov counts the numbers of lines that the program hits.\nThe percentage and the fact whether the package is  built successfully will reflect on the CodeCov and Travis badges.\n\n**Our package will be available for install from PyPI, and can be installed, updated, or removed using pip3**\n\n**Framework for software packaging**\n- We will package our software using the PyScaffold framework, as it will help facilitate configuration, versioning, and has some helpful extensions \nsuch as compatibility with Django and compatibility with Sphinx. For documentation, we will use the Sphinx framework.\n\n**Notes**\n- Our software will be designed in Python3 and will run in Python3 environments\n- Our software is pip installable: `pip install socialAD`","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-cd561589-5699-4d08-9d5f-4caaf8813064"}},{"cell_type":"markdown","source":"## Implementation\n\nImplementation of the _forward mode_ of automatic differentiation.\n\n* Core data structures:\n\nData will be stored in `numpy.array`. Numpy is the only external dependency of the package.\n\n* Classes:\n\n    * `forwardAD`: the main automatic differentiation object. The classes below are subclasses of `forwardAD`.\n        * Instantiate with a value and input into a function to find function value and derivative value.\n        * Attributes:\n            * `.der`: derivative as evaluated at the input value\n            * `.val`: function value as evaluated at the inpute value\n        * Example (to represent the function $3 + 2x$):\n            ```\n            >>> x = fowardAD(5)\n            >>> f = 3 + 2 * x\n            >>> print(f.val, f.der)\n            13.0 2.0\n            ```\n\n    * `e`: represents the natural number e. Inherits from `forwardAD`\n        * Instantiate with an open parenthesis to represent the natural number in a function: `e()`\n        * Example (to represent the function $3 + e^{x}$):\n            ```\n            >>> x = fowardAD(5)\n            >>> f = 3 + e() ** x\n            >>> print (f.val, f.der)\n            151.413159 148.413159\n            ```\n\n    * `log`: represents a logarithm. Defaults to natural logarithm (ln), but the base can be changed.\n        * Instantiate with functional input inside of `log()`\n        * Example (to represent the function $ln(3x) + 5$):\n            ```\n            >>> x = fowardAD(5)\n            >>> f = log(3 * x) + 5\n            >>> print (f.val, f.der)\n            7.708050201 0.2\n            ```\n        * Example (to represent the function $\\log_{10}(3x) + 5$):\n            ```\n            >>> x = fowardAD(5)\n            >>> f = log(3 * x, base = 10) + 5\n            >>> print (f.val, f.der)\n            6.17609125905 0.08685889638065\n            ```\n\n    * Basic trigonometric functions: `sin` (sine), `cos` (cosine), `tan` (tangent), `arcsin` (arcsine), `arccos` (arccosine), `arctan`(arctangent), `sinh` (hyperbolic sine), `cosh` (hyperbolic cosine), `tanh` (hyperbolic tangent)\n        * Trigonometric functions, all calculations done in radians\n        * Instantiate with functional input inside of trigonometric class\n        * Example (to represent the function $\\frac{\\sin(x+3)}{\\arcsin(-1)}$):\n            ```\n            >>> x = fowardAD(5)\n            >>> f = sin(x + 3) / arcsin(-1)\n            >>> print (f.val, f.der)\n            -0.6298 0.09263\n            ```\n            \n    * `optimize` (hasn't been implemented yet) \n        * Will find minima and maxima of various functions used in social science algorithms (can be applied in other fields as well)\n\n* Important methods and attributes:\n    * `forwardAD` and its subclasses (`e`,`log`,`sin`, etc.) have two key attributes\n        * `.val`: the function value evaluated at the provided input\n        * `.der`: the derivative value evaluated at the provided input\n    * In the future, the following methods will be implemented for our social science extension:\n        * `differential_equation`\n            * `seed` attribute\n            * `evaluate` method\n        * `equation`\n            * `f` attribute (converting the inputs to `numpy` formats)\n            * `jacobian` method\n        * `optimize` \n            * `domain` attribute\n            * `application` attribute\n            * `max` method: calculating maximum objective/loss function\n            * `min` method: calculating minimum objective/loss function\n\n\n* External dependencies:\n\n    * `numpy`\n\n* Elementary function conventions:\n\n    * The basic operations in python apply to our package. For example, addition can by symbolized with `+`, subtraction with `-`, multiplication with `*`, and division with `/`\n    * All multiplication must be made explicity with a `*` symbol.\n    * Raising to a power is symbolized with `**`\n    * The natural number, e, can be used by calling the `e()` class. To exponentiate to a power (ex: to the power of x), write: `e()**x`\n    * To write a logarithm, call the `log()` class. This class defaults to the natural logarith (base e), but the base can be changed by setting the `base` attribute to the desired base.\n    * To write trig functions, we have the following classes:\n        * `sin()` for sine\n        * `cos()` for cosine\n        * `tan()` for tangent\n        * `arcsin()` for arcsine\n        * `arccos()` for arccosine\n        * `arctan()` for arctangent\n        * `sinh()` for hyperbolic sine\n        * `cosh()` for hyperbolic cosine\n        * `tanh()` for hyperbolic tangent\n    \n    \n    * Two helpful examples:\n\n        * To write $f(x) = e^{x} - (2 - 6x - 3x^{5})$ we'd write: `f = e()**x - (2 - 6*x - 3 * x ** 5)`\n        * to write $f(x) = ln(x) + sin(x + 5) - \\frac{x^2}{\\log_{10}{(12x)}}$ we'd write: `f = log(x) + sin(x + 5) - (x**2) / (log(12 * x, base = 10))`\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-2a4c44d9-8e59-4a35-9092-67d62c9b43bf"}},{"cell_type":"markdown","source":"## Future Features\n\nTo build upon the work we've already done, we will implement classes that compute the inverse of the hyperbolic trig functions. Then, we will expand the classes we've already written to handle multiple inputs for multiple variables. \n\nLike the rest of our implementation in `forward` we will attempt to do this through duck-typing. In addition, we will modify our code such that (when requested) it outputs a Jacobian matrix as a numpy array, for cases with multiple input variables.\n\nFor our extension, we will implement another module called `social` that will compute maxima or minima of a loss or objective function for applications in social science.\n\nThe intent behind this extension is to provide ready resources for social science programmers, who often optimize loss functions for fitting growth models or inferential models.\n\nUnfortunately, algorithms for application in social science are often slow or difficult to use, due to the lack of programmers who focus in the social sciences (compared to other fields). Our package is unique in that it will provide a fast implementation that is marketed towards social scientists! \n\nHere is a list of planned methods:\n\n* In the future, the following methods will be implemented for our social science extension:\n    * `differential_equation`\n        * `seed` attribute\n        * `evaluate` method\n    * `equation`\n        * `f` attribute (converting the inputs to `numpy` formats)\n        * `jacobian` method\n    * `optimize` \n        * `domain` attribute\n        * `application` attribute\n        * `max` method: calculating maximum objective/loss function\n        * `min` method: calculating minimum objective/loss function\n\nWe will also add to our documentation some examples of package use that directly relate to common uses in social science.\n\nSome anticipated challenges are figuring out ways to handle multiple dimensions which don't reduce speed. In addition, another challenge we will face is making sure our optimization implementation gets as close to machine precision as possible.\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00008-5840143e-2395-4a42-85c4-95e148d5a16d"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00008-29866005-b0b4-43dd-a67a-3359ed4b64d4"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"63c1a46e-9fdc-454a-a6f6-31be175b0522","deepnote_execution_queue":[]}}