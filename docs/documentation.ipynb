{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation: socialAD\n",
    "\n",
    "*A root-finding and station point-finding package, powered by automatic differentiation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-f654ad6a-008b-4b21-ba58-b79505dfdfca",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "The software provides users with functions that find roots and stationary points of equations. The algorithms implemented are powered by automatic differentiation (forward mode). Automatic differentiation allows our alogrithms to solve derivatives quickly and to machine precision, which means our root and stationary finding algorithms are more efficient than many packages built on finite difference methods. \n",
    "\n",
    "The functions we provide - Newton's Method, Broyden's Method, and Gradient Descent, and Broyden–Fletcher–Goldfarb–Shanno (BFGS) - are widely applicable in many fields. For example, engineers use root-finding to optimize space given material constraints, data scientists use root-finding to inform machine learning models, physicists find stationary points to map particle motion, and economists find stationary points to describe changing markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-6b8a6822-8663-4b7c-ad5d-80c2b69ab4c0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Background: Root-Finding, Powered by Automatic Differentiation\n",
    "\n",
    "Our package provides functionality for:\n",
    "\n",
    "- **Newton's Method (root-finding)**\n",
    "    - Approximates roots by setting an initial guess, then iteratively subtracting the function value scaled by the derivative from the latest guess. This eventually provides very precise (if not exact) approximations of roots.\n",
    "    - For scalar non-vector functions: $x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}$\n",
    "    - For vector functions: $ x_{n+1}=x_{n}-J_{F}(x_{n})^{-1}F(x_{n})$\n",
    "\n",
    "\n",
    "- **Broyden's Method (root-finding)**\n",
    "    - This method is extremely similar to Newton's method, except that it employs finite differences after the first step of the Newton iteration. So, for a non-vector function, it computes finite differences at each iteration after the first. For vector functions, it computes a finitely-updated Jacobian at each iteration after the first.\n",
    "    - Mostly used in applications in which the Jacobian is computationally expensive to calculate. This method saves the machine from having to calculate a new Jacobian at each step.\n",
    "\n",
    "\n",
    "- **Gradient Descent (stationary point)**\n",
    "    - Common in the world of machine learning, gradient descent steps through the steepest gradient drop direction until the method finds a stationary point.\n",
    "    - Our implementation allows the user to change the steps size and to use backtrack line search.\n",
    "\n",
    "\n",
    "- **Broyden–Fletcher–Goldfarb–Shanno (BFGS) (stationary point)**\n",
    "    - Common in the world of machine learning, gradient descent steps through the steepest gradient drop direction until the method finds a stationary point.\n",
    "    - Our implementation allows the user to change the steps size and to use backtrack line search.\n",
    "\n",
    "\n",
    "- **Automatic Differentiation**\n",
    "    - Used to power our package, automatic differentiation (forward mode) uses the chain rule to step through complex functions and calculate derivatives. More background is given below.\n",
    "    - Users can use the forward mode of our automatic differentiation methods for their own purposes (outside of our root-finding algorithms), if desired\n",
    "    - Implementation for second-order derivatives is also included\n",
    "\n",
    "**Background on Automatic Differentiation and the Chain Rule**\n",
    "\n",
    "The chain rule is used to evaluate the derivative of a function by differentiating the outer function, multiplied by the derivative of the inner function, until all layers of the function are complete.\n",
    "\n",
    "Suppose we have a function $F(x) = f(g(x))$, then $F'(x) = f'(g(x))g'(x)$.\n",
    "\n",
    "Using Leibniz notation, the chain rule is represented as:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$\n",
    "\n",
    "\n",
    "We can traverse a function $f(x)$ by following what happens to its input, $x$. We evaluate from the innermost layer and work our way outwards. For each layer, we calculate its derivative and then feed that as the input to the next layer.\n",
    "\n",
    "Elementary functions (_eg._ $sin$, $cos$, $exp$, $log$) are common in AD applications. These functions usually differentiate to fixed solutions; therefore, we implement methods that evaluate these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-51911732-2c12-4dcb-9f21-2b2939bb6783",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## How to use socialAD\n",
    "\n",
    "Users can simply download our open-source package _socialAD_ via pip. \n",
    "\n",
    "\n",
    "```\n",
    "pip install socialAD\n",
    "```\n",
    "\n",
    "We have several modules that can be imported. `forward` for forward-mode of automatic differentiation. `root_finding` for Newton's Method and Broyden's Method. `gradient` for gradient descent. Importing numpy is also required. Here are the recommended import statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socialAD.forward as fw\n",
    "import socialAD.forward_pro as fwp\n",
    "import socialAD.root_finding as rf\n",
    "import socialAD.gradient as gd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root Finding**\n",
    "\n",
    "The following is a short demonstration of root-finding using Newton's Method and Broyden's Method. The following uses the broyden root-finding function `rf.broyden` and the newton root-finding function `rf.newton` to find the roots of two vector functions:\n",
    "\n",
    "$${F(x)=\\begin{bmatrix}{x^{2} - 25}\\\\{x - 5}\\\\\\end{bmatrix}}$$\n",
    "\n",
    "$${F(x,y)=\\begin{bmatrix}{x^{3} + y^{3} - 2}\\\\{16x^{3} - 16y^{3}}\\\\\\end{bmatrix}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For: [x**2 - 25, x - 5]\n",
      "Root from Broyden's Method:  [5.00000001]\n",
      "Root from Newton's Method:  [5.]\n",
      "\n",
      "\n",
      "For: [x**3 + y**3 - 2, 16*x**3 - 16*y**3]\n",
      "Root from Broyden's Method:  [1.00000002 1.00000002]\n",
      "Root from Newton's Method:  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import socialAD.root_finding as rf\n",
    "\n",
    "#First vector function (single variable)\n",
    "functions = [\"x**2 - 25\", \"x - 5\"]\n",
    "\n",
    "#Print roots\n",
    "print('For: [x**2 - 25, x - 5]')\n",
    "print(\"Root from Broyden's Method: \",rf.broyden(function = functions, variables = \"x\", init_values = [50])[0])\n",
    "print(\"Root from Newton's Method: \",rf.newton(function = functions, variables = \"x\", init_values = [50])[0])\n",
    "print('\\n')\n",
    "\n",
    "#Second vector function (mutliple variables)\n",
    "functions = [\"x**3 + y**3 - 2\", \"16*x**3 - 16*y**3\"]\n",
    "\n",
    "#Print roots\n",
    "print('For: [x**3 + y**3 - 2, 16*x**3 - 16*y**3]')\n",
    "print(\"Root from Broyden's Method: \", rf.broyden(function = functions, variables = [\"x\",\"y\"], init_values = [4,4])[0])\n",
    "print(\"Root from Newton's Method: \", rf.newton(function = functions, variables = [\"x\",\"y\"], init_values = [4,4])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "To find local minima rather than roots, one can use our gradient descent functions or Broyden–Fletcher–Goldfarb–Shanno (BFGS) functions. Along with gradient descent and BFGS, we allow users to use backtrack line search, which can find comparable solutions in fewer iterations. Here is a short demo of gradient descent and BFGS, with efficiency comparisons to backgrack solutions, for the function $f(x,y) = (x+5)^{2} + y^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "GRADIENT DESCENT\n",
      "============================================\n",
      "Iteration 600\n",
      "The local minimum occurs at [-4.99995648e+00  5.44058269e-06]\n",
      "============================================\n",
      "GD WITH BACKTRACK LINE SEARCH\n",
      "============================================\n",
      "Iteration 14\n",
      "The local minimum occurs at [-4.99999985e+00  1.82059120e-08]\n",
      "\n",
      "\n",
      "============================================\n",
      "BFGS\n",
      "============================================\n",
      "Iteration 66\n",
      "The local minimum occurs at [-4.99999684e+00  3.94944181e-07]\n",
      "============================================\n",
      "BFGS WITH BACKTRACK LINE SEARCH\n",
      "============================================\n",
      "Iteration 14\n",
      "The local minimum occurs at [-4.99999985e+00  1.83903042e-08]\n"
     ]
    }
   ],
   "source": [
    "import socialAD.gradient as gd\n",
    "\n",
    "#Set variables, function, and initial values\n",
    "variables = [\"x\",\"y\"]\n",
    "f = \"(x+5)**2 + y**2\"\n",
    "cur_x = [3, 1]\n",
    "rate = 0.01\n",
    "precision = 0.000001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "\n",
    "print('============================================')\n",
    "print('GRADIENT DESCENT')\n",
    "print('============================================')\n",
    "gd.gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters)\n",
    "\n",
    "print('============================================')\n",
    "print('GD WITH BACKTRACK LINE SEARCH')\n",
    "print('============================================')\n",
    "gd.gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters)\n",
    "print('\\n')\n",
    "\n",
    "print('============================================')\n",
    "print('BFGS')\n",
    "print('============================================')\n",
    "gd.BFGS(variables, f, cur_x, precision, max_iters)\n",
    "\n",
    "print('============================================')\n",
    "print('BFGS WITH BACKTRACK LINE SEARCH')\n",
    "print('============================================')\n",
    "gd.BFGS_backtrack(variables, f, cur_x, precision, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directly using automatic differentiation (AD) forward mode**\n",
    "\n",
    "To instantiate AD objects, we can simply call `fw.forwardAD(val)` with an initial function input value.\n",
    "\n",
    "Here is a quick demonstration of using the forward mode to solve the derivative of the function $$f(x) = e^{x} - (2 - 6x - 3x^{5})$$ at x = 5. Note: all multiplication must be made explicit.\n",
    "For more information on writing functions for our package, see the \"Elementary function conventions\" section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "00003-4c4bb330-d366-4e68-8a5d-ef07ffac81a5",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1605732291896,
    "output_cleared": false,
    "source_hash": "e2892992",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value:  9551.413159102576 \n",
      "Derivative value:  [9529.4131591]\n"
     ]
    }
   ],
   "source": [
    "import socialAD.forward as fw\n",
    "\n",
    "#Instantiate initial input to function\n",
    "x = fw.forwardAD(5)\n",
    "\n",
    "#Function (note: make multiplication explicit)\n",
    "f = fw.e()**x - (2 - 6*x - 3 * x ** 5)\n",
    "\n",
    "#Get function value at x = 5 and derivative at x = 5\n",
    "print('Function value: ', f.val, '\\nDerivative value: ', f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing second order derivatives**\n",
    "\n",
    "To find second-order derivatives using automatic differentiation, we can use the `fwp.forwardAD_pro` class from the `forward_pro` module.\n",
    "\n",
    "Here is a demonstration of the first and second order derivative for $ log (\\frac{1}{(tan^{-1}(x) x^{2}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Value:  2.8949439051408983\n",
      "1st Derivative:  -7.523384839521769\n",
      "2nd Derivative:  [20.48755358]\n"
     ]
    }
   ],
   "source": [
    "import socialAD.forward_pro as fwp\n",
    "\n",
    "x = fwp.forwardAD_pro(0.387)\n",
    "f = fwp.log(1/(fwp.arctan(x)*x**2))\n",
    "\n",
    "print(\"Function Value: \", f.func.val)\n",
    "print(\"1st Derivative: \", f.dera.val)\n",
    "print(\"2nd Derivative: \", f.dera.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-cd561589-5699-4d08-9d5f-4caaf8813064",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Software Organization\n",
    "\n",
    "\n",
    "**Directory Structure:**\n",
    "\n",
    "`docs`\n",
    "\n",
    "- Includes documentation\n",
    "\n",
    "`examples`\n",
    "\n",
    "- Includes examples of forward automatic differentiation and examples from our video presentation\n",
    "\n",
    "`socialAD`\n",
    "\n",
    "- Includes all modules\n",
    "- `forward.py`: forward mode of automatic differentiation (used by root and stationary point finding algorithms). Also includes second order derivative functionality.\n",
    "- `forward_pro.py`: second-order derivatives using the forward mode of automatic differentiation\n",
    "- `gradient.py`: functions for gradient descent and BFGS (with back-tracking)\n",
    "- `root_finding.py`: functions for root-finding, including Newton's Method and Broyden's Method\n",
    "\n",
    "`tests`\n",
    "- Includes all the tests\n",
    "- `broyden_tests.py`: tests for Broyden's Method implementation\n",
    "- `forwardAD_tests.py`: tests for automatic differentiation (forward mode)\n",
    "- `gradient_tests.py`: tests for gradient descent\n",
    "- `root_finding_tests.py`: tests for Newton's Method and Broyden's Method\n",
    "\n",
    "README.md\n",
    "\n",
    "- Includes install instructions, author names, and CodeCov/TravisCI buttons\n",
    "\n",
    "requirements.txt\n",
    "\n",
    "- Lists requirements for being able to successfully use our package\n",
    "\n",
    "\n",
    "**Our suite is tested on both TravisCI and CodeCov**\n",
    "Please check `.travis.yml` to see how the tests were performed. The big picture us the suite is built on Travis, and CodeCov runs the testing scripts in the `tests` folder. CodeCov counts the numbers of lines that the program hits.\n",
    "The percentage and the fact whether the package is built successfully will reflect on the CodeCov and Travis badges.\n",
    "\n",
    "**Our package is available for install from PyPI, and can be installed, updated, or removed using pip**\n",
    "\n",
    "`pip install socialAD`\n",
    "\n",
    "#### For developers\n",
    "\n",
    "First, clone the repository to your local machine:\n",
    "\n",
    "`$ git clone https://github.com/climate-change-is-real-python-dev/cs107-FinalProject.git`\n",
    "\n",
    "\n",
    "Go to the repository:\n",
    "\n",
    "`$ cd /path/to/cs107-FinalProject`\n",
    "\n",
    "\n",
    "##### Installing dependencies\n",
    "\n",
    "`$ pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "##### Running unit tests\n",
    "\n",
    "Say you have written an optimization function and would like to test it. You can create a unit test file called `optimization_tests.py` in the `tests` folder:\n",
    "\n",
    "`$ cd /path/to/cs107-FinalProject/tests`\n",
    "\n",
    "\n",
    "`$ touch optimization_tests.py`\n",
    "\n",
    "After creating test cases, head over to the top level and open `runtests.py`:\n",
    "\n",
    "`$ cd ..`\n",
    "\n",
    "\n",
    "`$ vi runtests.py`\n",
    "\n",
    "Add the following line to `runtests.py`:\n",
    "\n",
    "`$ runpy.run_path(path_name='tests/optimization_tests.py')`\n",
    "\n",
    "Stay in `cs107-FinalProject` and run:\n",
    "\n",
    "`$ chmod +x runtests.py`\n",
    "\n",
    "\n",
    "`$ ./runtests.py`\n",
    "\n",
    "Or this:\n",
    "\n",
    "`$ python runtests.py`\n",
    "\n",
    "\n",
    "**Notes**\n",
    "- Our software will be designed in Python3 and will run in Python3 environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-2a4c44d9-8e59-4a35-9092-67d62c9b43bf",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Automatic Differentiation Implementation\n",
    "\n",
    "**From `forward.py` module:**\n",
    "\n",
    "* Core data structures:\n",
    "\n",
    "    - Data will be stored in `numpy.array`. Numpy is the only external dependency of the package.\n",
    "\n",
    "* Classes:\n",
    "\n",
    "    * `forwardAD(val, numvar, idx)`: the main automatic differentiation object. The classes below are subclasses of `forwardAD`.\n",
    "        * Instantiate with a value and input into a function to find function value and derivative value.\n",
    "        * Attributes:\n",
    "            * `.der`: derivative as evaluated at the input value\n",
    "            * `.val`: function value as evaluated at the inpute value\n",
    "        * Example (to represent the function $3 + 2x$):\n",
    "            ```\n",
    "            >>> x = fowardAD(5)\n",
    "            >>> f = 3 + 2 * x\n",
    "            >>> print(f.val, f.der)\n",
    "            13.0 2.0\n",
    "            ```\n",
    "        * The `numvar` and `idx` options are used when setting up vector functions. \n",
    "            * `numvar` takes an int specifying the total number of variables in a vector function.\n",
    "            * `idx` takes an int specifying the index of the variable that the `forwardAD` object defines within a list of variables used in the vecot function.\n",
    "            * See `vector_func` implementation details (below) for more information.\n",
    "\n",
    "\n",
    "* `e`: represents the natural number e. Inherits from `forwardAD`\n",
    "        * Instantiate with an open parenthesis to represent the natural number in a function: `e()`\n",
    "        * Example (to represent the function $3 + e^{x}$):\n",
    "            ```\n",
    "            >>> x = fowardAD(5)\n",
    "            >>> f = 3 + e() ** x\n",
    "            >>> print (f.val, f.der)\n",
    "            151.413159 148.413159\n",
    "            ```\n",
    "\n",
    "* `log`: represents a logarithm. Defaults to natural logarithm (ln), but the base can be changed.\n",
    "    * Instantiate with functional input inside of `log()`\n",
    "    * Example (to represent the function $ln(3x) + 5$):\n",
    "        ```\n",
    "        >>> x = fowardAD(5)\n",
    "        >>> f = log(3 * x) + 5\n",
    "        >>> print (f.val, f.der)\n",
    "        7.708050201 0.2\n",
    "        ```\n",
    "    * Example (to represent the function $\\log_{10}(3x) + 5$):\n",
    "        ```\n",
    "        >>> x = fowardAD(5)\n",
    "        >>> f = log(3 * x, base = 10) + 5\n",
    "        >>> print (f.val, f.der)\n",
    "        6.17609125905 0.08685889638065\n",
    "        ```\n",
    "        \n",
    "* `sqrt`: represents the square root.\n",
    "    * Instantiate with functional input inside of `sqrt()`\n",
    "    * Example (to represent the function $\\sqrt{4x}$):\n",
    "        ```\n",
    "        >>> x = fowardAD(4)\n",
    "        >>> f = sqrt(4 * x)\n",
    "        >>> print (f.val, f.der)\n",
    "        4.0 0.5\n",
    "        ```\n",
    "        \n",
    "* `sigmoid`: represents the logistic function\n",
    "    * Instantiate with functional input inside of `sigmoid()`\n",
    "    * Example, to represent the function $sigmoid(4x) = \\frac{1}{1 + \\frac{1}{e^{4x}}}$:\n",
    "        ```\n",
    "        >>> x = fowardAD(4)\n",
    "        >>> f = sigmoid(4 * x)\n",
    "        >>> print (f.val, f.der)\n",
    "        0.99999997 0.00000045\n",
    "        ```\n",
    "\n",
    "    * Basic trigonometric functions: `sin` (sine), `cos` (cosine), `tan` (tangent), `arcsin` (arcsine), `arccos` (arccosine), `arctan`(arctangent), `sinh` (hyperbolic sine), `cosh` (hyperbolic cosine), `tanh` (hyperbolic tangent)\n",
    "        * Trigonometric functions, all calculations done in radians\n",
    "        * Instantiate with functional input inside of trigonometric class\n",
    "        * Example (to represent the function $\\frac{\\sin(x+3)}{\\arcsin(-1)}$):\n",
    "            ```\n",
    "            >>> x = fowardAD(5)\n",
    "            >>> f = sin(x + 3) / arcsin(-1)\n",
    "            >>> print (f.val, f.der)\n",
    "            -0.6298 0.09263\n",
    "            ```\n",
    "            \n",
    "    * `vector_func(*args)`: Takes in `forwardAD` objects as inputs and represents them as a vector function. Calculates values and the Jacobian.\n",
    "        * Instantiate with inputted `forwardAD` objects. Stores values and Jacobian as attributes, which can be accessed.\n",
    "        * Attributes:\n",
    "            * `.jacobian_array`: the Jacobian for the vector function, stored as a numpy array\n",
    "            * `.values_array`: function values of the vector function, stored as a numpy array\n",
    "        * Methods:\n",
    "            * `.jacobian()`: returns Jacobian (as numpy array)\n",
    "            * `.values()`: returns function values (as numpy array)\n",
    "        * Example to find Jacobina of the vector function ${F(x)=\\begin{bmatrix}{xy}\\\\{x^{2} + y^{2}}\\\\\\end{bmatrix}}$\n",
    "            ```\n",
    "            >>> a = np.array([3]) # Value of x to be evaluated \n",
    "            >>> x = fw.forwardAD(a, numvar = 2, idx = 0)\n",
    "\n",
    "            >>> b = np.array([2]) # Value of y to be evaluated \n",
    "            >>> y = fw.forwardAD(b, numvar = 2, idx = 1)\n",
    "\n",
    "            >>> #Functions that will compose our vector functions\n",
    "            >>> f1 = x*y\n",
    "            >>> f2 = x**2 + y**2\n",
    "\n",
    "            >>> #Stores as vector function\n",
    "            >>> vector_function = fw.vector_func(f1, f2)\n",
    "\n",
    "            >>> #Returns jacobian\n",
    "            >>> print(vector_function.jacobian())\n",
    "            [[2. 3.]\n",
    "            [6. 4.]]\n",
    "            ```\n",
    "\n",
    "\n",
    "* External dependencies:\n",
    "\n",
    "    * `numpy`\n",
    "\n",
    "* Elementary function conventions:\n",
    "\n",
    "    * The basic operations in python apply to our package. For example, addition can by symbolized with `+`, subtraction with `-`, multiplication with `*`, and division with `/`\n",
    "    * All multiplication must be made explicity with a `*` symbol.\n",
    "    * Raising to a power is symbolized with `**`\n",
    "    * Negation can be done by multiplying to -1: `(-1)*x`\n",
    "    * Square roots can be done by calling the `sqrt()` class: `sqrt(x)`\n",
    "    * The natural number, e, can be used by calling the `e()` class. To exponentiate to a power (ex: to the power of x), write: `e()**x`\n",
    "    * To write a logarithm, call the `log()` class. This class defaults to the natural logarith (base e), but the base can be changed by setting the `base` attribute to the desired base.\n",
    "    * To get the logistic function, call `sigmoid()` class: `sigmoid(x)`\n",
    "    * To write trig functions, we have the following classes:\n",
    "        * `sin()` for sine\n",
    "        * `cos()` for cosine\n",
    "        * `tan()` for tangent\n",
    "        * `arcsin()` for arcsine\n",
    "        * `arccos()` for arccosine\n",
    "        * `arctan()` for arctangent\n",
    "        * `sinh()` for hyperbolic sine\n",
    "        * `cosh()` for hyperbolic cosine\n",
    "        * `tanh()` for hyperbolic tangent\n",
    "    \n",
    "    \n",
    "- Two helpful examples:\n",
    "    - To write $f(x) = e^{x} - (2 - 6x - 3x^{5})$ we'd write: `f = e()**x - (2 - 6*x - 3 * x ** 5)`\n",
    "    - To write $f(x) = ln(x) + sin(x + 5) - \\frac{x^2}{\\log_{10}{(12x)}}$ we'd write: `f = log(x) + sin(x + 5) - (x**2) / (log(12 * x, base = 10))`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-5840143e-2395-4a42-85c4-95e148d5a16d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Extension Implementation: Root-Finding Algorithms,  Stationary Point Finding Algorithms, and Second Order Derivatives\n",
    "\n",
    "For background on the motivation and mathematics behind our extension, see the \"background\" section at the beginning of the documentation.\n",
    "\n",
    "**From the `root_finding.py` module:**\n",
    "\n",
    "*Newton's Method*\n",
    "\n",
    "- Core Function: `newton(function, variables, init_values):`\n",
    "\n",
    "    - Performs Newton's method to find roots of scalar and vector functions\n",
    "    - Inputs: `function`: a function (as a string) or a vector function (as a list of strings), `variables`: the variables used in the fuction (as a list of strings), and `init_values` initial root guesses for each variable (as a list of int or floats)\n",
    "    - Returns: A list with two components:\n",
    "        - A numpy array with the roots for each variable\n",
    "        - A numpy array with the approximate function values at those roots (should be close to zero)\n",
    "    - Computational details: uses automatic differentiation to compute derviatives for scalar functions, and Jacobians for vector functions. Uses `1e-7` as the threshold for finding a zero.\n",
    "    - Example use on the vector function ${F(x,y)=\\begin{bmatrix}{x^{3} + y^{3} - 2}\\\\{16x^{3} - 16y^{3}}\\\\\\end{bmatrix}}$:\n",
    "    \n",
    "```\n",
    ">>> functions = [\"x**3 + y**3 - 2\", \"16*x**3 - 16*y**3\"]\n",
    ">>> print(\"Root from Newton's Method: \", newton(function = functions, variables = [\"x\",\"y\"], init_values = [4,4])[0])\n",
    "Root from Newton's Method:  [1. 1.]\n",
    "```\n",
    "    \n",
    "*Broyden's Method*\n",
    "\n",
    "- Core Function: `broyden(function, variables, init_values):`\n",
    "\n",
    "    - Performs Broyden's method to find roots of scalar and vector functions\n",
    "    - Inputs: `function`: a function (as a string)mor a vector function (as a list of strings), `variables`: the variables used in the fuction (as a list of strings), and `init_values` initial root guesses for each variable (as a list of int or floats)\n",
    "    - Returns: A list with two components:\n",
    "        - A numpy array with the roots for each variable\n",
    "        - A numpy array with the approximate function values at those roots (should be close to zero)\n",
    "    - Computational details: uses automatic differentiation to compute derviatives for scalar functions, and Jacobians for vector functions. Finds derivative or Jacobian on first iteration, then uses finite difference of finite Jacobian updates thereafter. Uses `1e-7` as the threshold for finding a zero.\n",
    "    - Example use on the vector function ${F(x,y)=\\begin{bmatrix}{x^{3} + y^{3} - 2}\\\\{16x^{3} - 16y^{3}}\\\\\\end{bmatrix}}$:\n",
    "    \n",
    "```\n",
    ">>> functions = [\"x**3 + y**3 - 2\", \"16*x**3 - 16*y**3\"]\n",
    ">>> print(\"Root from Broyden's Method: \", broyden(function = functions, variables = [\"x\",\"y\"], init_values = [4,4])[0])\n",
    "Root from Broyden's Method:  [1.00000002 1.000000002]\n",
    "```\n",
    "\n",
    "**From the `forward_pro.py` module:**\n",
    "\n",
    "*Second-order derivatives, using the forward mode of automatic differentiation*\n",
    "\n",
    "Core Class: `forwardAD_pro(val=None, der_vector=None, numvar = 1, idx = 0, func=None, dera=None):`\n",
    "\n",
    "- Performs second order derivatives, using forward mode of automatic differentiation\n",
    "- Inputs: `val`: a variable value to instantiate the object, `der_vector`: for vector functions, a list of seeds to set, `numvar`: the number of variables used in the expression, `idx`: takes an int specifying the index of the variable that the object defines within a list of variables used in vector_func, `func`: a prior `forwardAD` class object that defines the function, `dera`: the symbolic derivative of the prior function\n",
    "* Attributes:\n",
    "    * `.func`: function in symbolic form, a `forwardAD` object\n",
    "    * `.dera`: The first order derivitive, also a `forwardAD` object\n",
    "- Computational details: uses automatic differentiation to compute second-order derviatives for scalar functions (including with multiple variables)\n",
    "- Example use on function: $ log (\\frac{1}{(tan^{-1}(x) x^{2}})$\n",
    "\n",
    "```\n",
    ">>> x = fwp.forwardAD_pro(0.387)\n",
    ">>> f = log(1/(arctan(x)*x**2))\n",
    "\n",
    ">>> print(\"Function Value: \", f.func.val)\n",
    ">>> print(\"1st Derivative: \", f.dera.val)\n",
    ">>> print(\"2nd Derivative: \", f.dera.der)\n",
    "\n",
    "Function Value:  -1.5055671924729173\n",
    "1st Derivative:  1.7564520982201164\n",
    "2nd Derivative:  [-6.08601388]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**From the `gradient.py` module:**\n",
    "\n",
    "*Gradient Descent, Broyden–Fletcher–Goldfarb–Shanno (BFGS), and backtracking versions*\n",
    "\n",
    "Core Functions: \n",
    "- `gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters):`\n",
    "- `gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters):`\n",
    "- `BFGS(variables, f, cur_x, precision, max_iters):`\n",
    "- `BFGS_backtrack(variables, f, cur_x, precision, max_iters):`\n",
    "\n",
    "    - Perform gradient descent, gradient descent backtracking, Broyden–Fletcher–Goldfarb–Shanno (BFGS), and BFGS backtracking to find local minima in scalar or vector functions\n",
    "    - Inputs: `variables`: the variables used in the fuction (as a list of strings), `f`: a function (as a string), `cur_x`: initial values from which to start descent, `precision`: precision at which iterations should stop, `previous_step_size`: previous step size,  `max_iters`: maximum step iterations before stopping\n",
    "    - Output: Prints two components\n",
    "        - The number of iterations it took to reach the local minima\n",
    "        - The value(s) of the local minima\n",
    "    - Computational details: uses automatic differentiation to compute derviatives for scalar functions, and Jacobians for vector functions. User-defined threshold (`precision`) at which iterations stop.\n",
    "    - Example use on the function $f(x,y) = (x+5)^{2} + y^{2}$ is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "GRADIENT DESCENT\n",
      "============================================\n",
      "Iteration 600\n",
      "The local minimum occurs at [-4.99995648e+00  5.44058269e-06]\n",
      "============================================\n",
      "GD WITH BACKTRACK LINE SEARCH\n",
      "============================================\n",
      "Iteration 14\n",
      "The local minimum occurs at [-4.99999985e+00  1.82059120e-08]\n",
      "\n",
      "\n",
      "============================================\n",
      "BFGS\n",
      "============================================\n",
      "Iteration 66\n",
      "The local minimum occurs at [-4.99999684e+00  3.94944181e-07]\n",
      "============================================\n",
      "BFGS WITH BACKTRACK LINE SEARCH\n",
      "============================================\n",
      "Iteration 14\n",
      "The local minimum occurs at [-4.99999985e+00  1.83903042e-08]\n"
     ]
    }
   ],
   "source": [
    "import socialAD.gradient as gd\n",
    "\n",
    "#Set variables, function, and initial values\n",
    "variables = [\"x\",\"y\"]\n",
    "f = \"(x+5)**2 + y**2\"\n",
    "cur_x = [3, 1]\n",
    "rate = 0.01\n",
    "precision = 0.000001\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "\n",
    "print('============================================')\n",
    "print('GRADIENT DESCENT')\n",
    "print('============================================')\n",
    "gd.gradient_descent(variables, f, cur_x, rate, precision, previous_step_size, max_iters)\n",
    "\n",
    "print('============================================')\n",
    "print('GD WITH BACKTRACK LINE SEARCH')\n",
    "print('============================================')\n",
    "gd.gd_backtrack(variables, f, cur_x, precision, previous_step_size, max_iters)\n",
    "print('\\n')\n",
    "\n",
    "print('============================================')\n",
    "print('BFGS')\n",
    "print('============================================')\n",
    "gd.BFGS(variables, f, cur_x, precision, max_iters)\n",
    "\n",
    "print('============================================')\n",
    "print('BFGS WITH BACKTRACK LINE SEARCH')\n",
    "print('============================================')\n",
    "gd.BFGS_backtrack(variables, f, cur_x, precision, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guidance for writing your functions as inputs:\n",
    "\n",
    "* The basic operations in python apply to our package. For example, addition can by symbolized with `+`, subtraction with `-`, multiplication with `*`, and division with `/`\n",
    "* All multiplication must be made explicity with a `*` symbol.\n",
    "* Raising to a power is symbolized with `**`\n",
    "* Negation can be done by multiplying to -1: `(-1)*x`\n",
    "* Square roots can be done by calling the `sqrt()` class: `sqrt(x)`\n",
    "* The natural number, e, can be used by calling the `e()` class. To exponentiate to a power (ex: to the power of x), write: `e()**x`\n",
    "* To write a logarithm, call the `log()` class. This class defaults to the natural logarith (base e), but the base can be changed by setting the `base` attribute to the desired base.\n",
    "* To get the logistic function, call `sigmoid()` class: `sigmoid(x)`\n",
    "* To write trig functions, we have the following classes:\n",
    "    * `sin()` for sine\n",
    "    * `cos()` for cosine\n",
    "    * `tan()` for tangent\n",
    "    * `arcsin()` for arcsine\n",
    "    * `arccos()` for arccosine\n",
    "    * `arctan()` for arctangent\n",
    "    * `sinh()` for hyperbolic sine\n",
    "    * `cosh()` for hyperbolic cosine\n",
    "    * `tanh()` for hyperbolic tangent\n",
    "\n",
    "    \n",
    "- Two helpful examples:\n",
    "    - To write $f(x) = e^{x} - (2 - 6x - 3x^{5})$ we'd write: `f = \"e()**x - (2 - 6*x - 3 * x ** 5)\"`\n",
    "    - To write $f(x) = ln(x) + sin(x + 5) - \\frac{x^2}{\\log_{10}{(12x)}}$ we'd write: `\"f = log(x) + sin(x + 5) - (x**2) / (log(12 * x, base = 10))\"`\n",
    "\n",
    "\n",
    "* External dependencies:\n",
    "\n",
    "    * `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broader Impact Statement\n",
    "\n",
    "There are multiple ways to use our package for greater scientific and social good. There are also ways to misuse our package in these respects.\n",
    "\n",
    "The functions we provide are widely applicable in many fields. For example, engineers use root-finding to optimize space given material constraints, data scientists use root-finding to inform machine learning models, physicists find stationary points to map particle motion, and economists find stationary points to describe changing markets. To the extent that fields may use our package to spur innovation, solve problems that help humanity, create a more efficient workforce and economy, and work towards a more advanced and just world - we will be proud our package made some contribution to these efforts.\n",
    "\n",
    "However, alongside innovation can come consequences. For example, our gradient descement implementations may be used in machine learning implementations. Machine learning algorithms have introduced bias in [policing and in court systems](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), taken [jobs away](https://time.com/5876604/machines-jobs-coronavirus/) from the service industry (and others), and accelerated new ways of [illegal hacking](https://techhq.com/2020/09/how-hackers-are-weaponizing-artificial-intelligence/). In addition, fields of science that could utilize our package and that spur technological innovation (such as physics) can also spur great human loss with that innovation (such as the invention of the atomic bomb - as one historic example). We strongly encourage those who use our package to consider the consequences of their work, especially as they apply their work to contexts that are prone to the types of issues mentioned above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Inclusivity\n",
    "\n",
    "Our package is open-source on GitHub, and pull requests can be made by anyone who has a free GitHub account. The package authors (Tao, Jenny, Ju, and Dash) will approve pull requests after collectively discussing the impact of those pull requests. We will review pull requests without viewing the personal details of user accounts who make them, thus preventing some level of bias in our decisions.\n",
    "\n",
    "However, there may be underlying barriers that prevent certain populations from easily contributing to our work.\n",
    "\n",
    "For example, our core coding group primarily speaks English. Those who speak primarily other languages may have a difficult time submitting requests and interpreting/responding to feedback comments. To handle this issue, we will use google translate and other newly developed translation tools to provide feedback in the same language as is presented to us during the feedback process. While these translations are not perfect, they will be better than no translation. We will consult language experts for translation help when applicable.\n",
    "\n",
    "In addition, a general digital divide in access to technology and access to the education that would allow for skill in tools to contribute will prevent some from contributing to our package. The authors of this package commit to contributing to nonprofits that help break down this digital and educational divide. Specifically, we will make contributions to [Partners Bridging the Digital Divide](https://www.pbdd.org/) on an annual basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "In the future, we would like to incorporate more root finding algorithms into our package (including those that don't necessarily need derivatives), including inverse interpolation, Brent's method, and Steffensen's method. This would expand our package into complex values, which also have broad applications in many fields.\n",
    "\n",
    "In accordance with our name, we would also like to develop applications of our software specific to the social sciences. Specifically, the field of economics could use our package in multiple ways. For instance, we could build examples and methods geared specifically towards finding rates of change in macroeconomic models and optimizing profit/cost functions for businesses. In addition, our package's gradient descent implementation could be used to support [current research in Fisher markets](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/prop_response.pdf)."
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "63c1a46e-9fdc-454a-a6f6-31be175b0522",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
